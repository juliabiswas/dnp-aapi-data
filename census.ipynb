{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports & global vars"
      ],
      "metadata": {
        "id": "cwDvZPvIt25I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYWOXvW-U7rF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C9XiNqiibIG"
      },
      "outputs": [],
      "source": [
        "!pip install dython\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from scipy.stats import rankdata\n",
        "from scipy.stats import chi2_contingency\n",
        "from dython import nominal as nom\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoKfe2h2icXU"
      },
      "outputs": [],
      "source": [
        "weight = \"PWGTP\"\n",
        "housing_identifier = \"SERIALNO\"\n",
        "person_identifier = \"SPORDER\"\n",
        "lim = \"limiter\"\n",
        "weight_col = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7WlIqwW242p"
      },
      "outputs": [],
      "source": [
        "vars = [\"PINCP\", \"PWGTP\", \"SERIALNO\", \"HHLDRAGEP\", \"BROADBND\", \"SMARTPHONE\", \"TEL\",\n",
        "        \"TABLET\", \"LAPTOP\", \"ACCESSINET\", \"COMPOTHX\", \"OTHSVCEX\", \"SATELLITE\", \"HISPEED\",\n",
        "        \"DIALUP\", \"SSIP\", \"ELEP\", \"RAC2P\", \"RAC3P\", \"RAC1P\", \"RACNUM\",\n",
        "        \"HINS1\", \"HINS2\", \"HINS3\", \"HINS4\", \"HINS5\", \"HINS6\", \"HINS7\", \"HICOV\",\n",
        "        \"PRIVCOV\", \"PUBCOV\", \"FHINS4C\", \"FHINS3C\", \"FHINS5C\", \"WATP\", \"MHP\", \"RETP\",\n",
        "        \"SSP\", \"HINCP\", \"RMSP\", \"INTP\", \"SEMP\",\n",
        "        \"SMP\", \"PERNP\", \"PAP\", \"GASP\", \"WKWN\", \"WAGP\", \"FULP\", \"SMOCP\",\n",
        "        \"FINCP\", \"OIP\", \"TAXAMT\", \"CONP\", \"INSP\", \"OCPIP\", \"GRNTP\", \"MRGP\",\n",
        "        \"VALP\", \"BDSP\", \"NOC\", \"NP\", \"NRC\", \"SPORDER\", \"NPF\", \"RNTP\",\n",
        "        \"WKHP\", \"POVPIP\", \"GRPIP\", \"JWMNP\", \"ADJHSG\", \"ADJINC\", \"MV\",\n",
        "        \"FPARC\", \"DRIVESP\", \"RACSOR\", \"JWAP\", \"R60\", \"RELSHIPP\", \"VACDUR\",\n",
        "        \"MLPIK\", \"PLM\", \"VPS\", \"DEAR\", \"R18\", \"MLPJ\", \"GCL\", \"STOV\",\n",
        "        \"ELEFP\", \"WATFP\", \"YOEP\", \"SMX\", \"MLPCD\", \"ANC2P\",\n",
        "        \"WRK\", \"POBP\", \"RACAIAN\", \"HHT2\", \"MLPFG\", \"FOD1P\",\n",
        "        \"FOD2P\", \"NAICSP\", \"INDP\", \"WAOB\", \"SOCP\", \"GASFP\",\n",
        "        \"HIMRKS\", \"HOTWAT\", \"NWLA\", \"JWTRNS\", \"REFR\", \"PSF\",\n",
        "        \"DECADE\", \"FULFP\", \"MRGT\", \"VACOTH\", \"LANP\", \"ANC1P\",\n",
        "        \"TEN\", \"POWPUMA\", \"PLMPRP\", \"CPLT\", \"YRBLT\", \"DRAT\",\n",
        "        \"NR\", \"MRGX\", \"MARHYP\", \"SINK\", \"MARHT\",\n",
        "        \"WIF\", \"HISP\", \"MAR\", \"SCHL\", \"NWLK\", \"DPHY\", \"DEYE\", \"MIGSP\",\n",
        "        \"HHLANP\", \"PARTNER\", \"RACNH\", \"WKL\", \"VEH\", \"DDRS\", \"MIGPUMA\",\n",
        "        \"LNGI\", \"QTRBIR\", \"SFN\", \"RACBLK\", \"MLPH\", \"ESR\", \"NPP\", \"DIS\",\n",
        "        \"HHLDRRAC1P\", \"MLPB\", \"DOUT\", \"SCH\", \"RACPI\",\n",
        "        \"POWSP\", \"ANC\", \"MIL\", \"OC\", \"HUGCL\", \"RWAT\", \"HHLDRHISP\", \"RESMODE\",\n",
        "        \"MARHW\", \"SFR\", \"ESP\", \"RACASN\", \"MLPE\", \"OCCP\", \"MARHD\", \"SCHG\",\n",
        "        \"MRGI\", \"MIG\", \"MSP\", \"FER\", \"MULTG\", \"WORKSTAT\", \"MARHM\", \"KIT\",\n",
        "        \"GCR\", \"HUPARC\", \"GCM\", \"ACR\", \"PAOC\", \"RNTM\", \"DRATX\", \"FS\",\n",
        "        \"SVAL\", \"RACWHT\", \"NWAB\", \"HUPAOC\", \"R65\", \"RC\", \"BATH\", \"SEX\",\n",
        "        \"HFL\", \"WKEXREL\", \"VACS\", \"HHL\", \"SRNT\", \"NWAV\", \"NWRE\", \"BLD\",\n",
        "        \"LANX\", \"MLPA\", \"HHT\", \"DREM\", \"COW\", \"HUPAC\", \"AGS\", \"ENG\",\n",
        "        \"JWRIP\", \"JWDP\", \"NATIVITY\", \"NOP\", \"AGEP\", \"CITWP\", \"CIT\"]\n",
        "ordinal = [\"IMMIG\", \"PARENT_EMP\", \"HLTH_INSUR\", \"INTERNET_ACC\", \"DEVICE_ACC\",\n",
        "           \"QTRBIR\", \"R60\", \"ACR\", \"AGS\", \"BLD\", \"ENG\", \"GCM\", \"LNGI\",\n",
        "           \"WIF\", \"WKL\", \"MARHT\", \"SCH\", \"SCHG\", \"SCHL\", \"DRAT\", \"DECADE\",\n",
        "           \"YOEP\", \"MARHYP\", \"VACDUR\", \"YRBLT\", \"MV\"]\n",
        "nominal = [\"FPARC\", \"COW\", \"DDRS\", \"DPHY\", \"DREM\", \"ESR\", \"GCL\", \"HFL\", \"HHL\",\n",
        "           \"HHT\", \"HUGCL\", \"HUPAC\", \"HUPAOC\", \"HUPARC\", \"LANX\", \"MAR\", \"MLPA\",\n",
        "           \"MLPB\", \"MLPE\", \"MLPH\", \"MLPJ\",\n",
        "          \"MSP\", \"NPP\", \"NR\", \"OC\", \"PAOC\", \"PSF\", \"R18\",\n",
        "          \"R65\", \"RC\", \"RNTM\", \"SEX\", \"SFN\", \"SFR\",\n",
        "          \"SRNT\", \"SVAL\", \"VACS\", \"WKEXREL\", \"WORKSTAT\", \"PARTNER\",\n",
        "          \"VPS\", \"GCR\", \"FS\", \"INSP\", \"DRATX\", \"DOUT\", \"DIS\",\n",
        "          \"DEYE\", \"DEAR\", \"BATH\", \"KIT\", \"MARHM\", \"MARHW\",\n",
        "          \"MIG\", \"MRGI\", \"MRGT\", \"MRGX\", \"MULTG\", \"REFR\", \"RWAT\",\n",
        "          \"SINK\", \"SMX\", \"STOV\", \"TEN\", \"WRK\", \"PLM\", \"FER\",\n",
        "          \"MIL\", \"RESMODE\", \"MLPCD\", \"MLPFG\", \"PLMPRP\", \"HOTWAT\",\n",
        "          \"WAOB\", \"ELEFP\", \"FULFP\", \"GSFP\", \"WATFP\", \"CPLT\",\n",
        "          \"HHT2\", \"HIMRKS\", \"JWTRNS\", \"RELSHIPP\", \"ANC\", \"ANC1P\",\n",
        "          \"ANC2P\", \"FOD1P\", \"FOD2P\", \"HHLANP\", \"INDP\", \"LANP\",\n",
        "          \"MARHD\", \"HHLDRHISP\", \"HHLDRRAC1P\", \"MIGPUMA\", \"MIGSP\",\n",
        "          \"MLPIK\", \"OCCP\", \"POBP\", \"PWPUMA\", \"POWSP\", \"SOCP\",\n",
        "          \"VACOTH\"]\n",
        "continuous = [\"HHLDRAGEP\", \"SSIP\", \"ELEP\", \"RAC2P\", \"RAC3P\",\n",
        "              \"RAC1P\", \"RACNUM\", \"WATP\", \"MHP\", \"RETP\",\n",
        "              \"SSP\", \"HINCP\", \"RMSP\", \"INTP\", \"SEMP\",\n",
        "              \"SMP\", \"PERNP\", \"PAP\", \"GASP\", \"WKWN\",\n",
        "              \"WAGP\", \"FULP\", \"SMOCP\", \"FINCP\", \"OIP\",\n",
        "              \"TAXAMT\", \"CONP\", \"OCPIP\", \"GRNTP\", \"MRGP\",\n",
        "              \"VALP\", \"BDSP\", \"NOC\", \"NP\", \"NRC\",\n",
        "              \"NPF\", \"RNTP\", \"WKHP\", \"POVPIP\", \"JWMNP\",\n",
        "              \"DRIVESP\", \"JWAP\", \"GASFP\", \"POWPUMA\", \"HISP\",\n",
        "              \"VEH\", \"JWRIP\", \"JWDP\", \"AGEP\", \"GRPIP\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data import functions\n",
        "\n",
        "data: the census bureauâ€™s american community survey (acs) public use microdata sample (pums)"
      ],
      "metadata": {
        "id": "AqqXz5sKt7x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main functions"
      ],
      "metadata": {
        "id": "RSfUTcH8vNpS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dha9-i0uxQ6q"
      },
      "outputs": [],
      "source": [
        "def import_census_data(variables, target, year, limiters, ca_limit, drop,\n",
        "                       nominal, ordinal, continuous):\n",
        "  '''\n",
        "  limiters: specified values to limit data requested\n",
        "  '''\n",
        "\n",
        "  vars = variables[:]\n",
        "\n",
        "  base_request = \"https://api.census.gov/data/\" + year + \"/acs/acs1/pums?get=\"\n",
        "  base_request += weight + \",\" + target\n",
        "  base_request += \",\" + housing_identifier + \",\" + person_identifier\n",
        "  request = base_request\n",
        "\n",
        "  #remove base vars\n",
        "  vars.remove(target)\n",
        "  vars.remove(weight)\n",
        "  vars.remove(housing_identifier)\n",
        "  vars.remove(person_identifier)\n",
        "\n",
        "  for v in drop:\n",
        "    vars.remove(v)\n",
        "\n",
        "  dfs = []\n",
        "\n",
        "  for i in range(len(limiters)):\n",
        "    l = limiters[i]\n",
        "    request += l\n",
        "\n",
        "    if ca_limit:\n",
        "      request += \"&ucgid=0400000US06\"\n",
        "\n",
        "    #get target/weight/ids\n",
        "    response = requests.get(request)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    df = df.rename(columns=df.iloc[0]).loc[1:].reset_index()\n",
        "\n",
        "    df[lim] = i\n",
        "    dfs.append(df)\n",
        "    request = base_request\n",
        "\n",
        "  data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "  #weight\n",
        "  data[weight] = data[weight].astype(int)\n",
        "  global weight_col\n",
        "  weight_col = data[weight]\n",
        "\n",
        "  target_col = data[target]\n",
        "  housing_identifier_col = data[housing_identifier]\n",
        "  person_identifier_col = data[person_identifier]\n",
        "  lim_col = data[lim]\n",
        "\n",
        "  if ca_limit:\n",
        "    data['ST'] = data['ST'].astype(int)\n",
        "  for col in drop:\n",
        "    data[col] = data[col].astype(int)\n",
        "\n",
        "  #dropping uneeded vars\n",
        "  to_drop = [\"NWAB\", \"NWAV\", \"NWLA\", \"NWLK\", \"NWRE\"] #ESR recoded them so can drop\n",
        "  to_drop.extend([\"ADJINC\", \"ADJHSG\"]) #only one unique value so don't need\n",
        "\n",
        "  #indp = based on industry codes, naicsp = based on NAICS codes (indp is\n",
        "  #derived from naicsp and is less detailed to protect individual respondents)\n",
        "  #indp also has higher correlation with income so choosing to keep indp over naicsp\n",
        "  to_drop.append(\"NAICSP\")\n",
        "\n",
        "  vars = [x for x in vars if x not in to_drop]\n",
        "  ordinal = [x for x in ordinal if x not in to_drop]\n",
        "  nominal = [x for x in nominal if x not in to_drop]\n",
        "  continuous = [x for x in continuous if x not in to_drop]\n",
        "\n",
        "  #requesting all data\n",
        "  curr = 0\n",
        "  while curr < len(vars):\n",
        "    data, curr, vars, ordinal, nominal, continuous = request_vars_and_merge(vars, data,\n",
        "                                                                            base_request,\n",
        "                                                                            limiters, target,\n",
        "                                                                            4, curr, target_col,\n",
        "                                                                            housing_identifier_col,\n",
        "                                                                            person_identifier_col,\n",
        "                                                                            lim_col, ca_limit,\n",
        "                                                                            nominal, ordinal,\n",
        "                                                                            continuous, drop)\n",
        "\n",
        "  data.drop(columns=[lim, 'index'], axis=1, inplace=True)\n",
        "  data.to_csv('/content/drive/My Drive/data.csv', index=False) #save to csv\n",
        "\n",
        "  return data, vars, ordinal, nominal, continuous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2CZsx5b07nS"
      },
      "outputs": [],
      "source": [
        "def request_vars_and_merge(vars, df, base_request, limiters, target, num_vars, curr,\n",
        "                           target_col, housing_identifier_col,\n",
        "                           person_identifier_col, lim_col, ca_limit, nominal,\n",
        "                           ordinal, continuous, drop):\n",
        "  '''\n",
        "  df: existing df\n",
        "  base_request: request for weight, target, & identifiers\n",
        "  limiter: any limiters on the data requested (e.g. state)\n",
        "  target: target variable\n",
        "  num_vars: vars in base_request\n",
        "  target_col: target var data\n",
        "  housing_identifier_col: housing ids\n",
        "  person_identifier_col: person ids within a household\n",
        "  '''\n",
        "\n",
        "  #get new vars\n",
        "  new_df, curr = request_vars(vars, base_request, limiters, target, num_vars,\n",
        "                              curr, ca_limit, nominal, ordinal, continuous)\n",
        "  new_df.loc[:,target] = target_col\n",
        "  new_df.loc[:,weight] = weight_col\n",
        "  new_df.loc[:,housing_identifier] = housing_identifier_col\n",
        "  new_df.loc[:,person_identifier] = person_identifier_col\n",
        "  new_df.loc[:,lim] = lim_col\n",
        "\n",
        "  #merge with existing df\n",
        "  on_cols = [housing_identifier, person_identifier, weight, target, lim]\n",
        "  if ca_limit:\n",
        "    on_cols.append(\"ST\")\n",
        "  on_cols.extend(drop)\n",
        "  df = pd.merge(df, new_df, on=on_cols)\n",
        "  df.drop_duplicates(keep='first', inplace=True, ignore_index=True)\n",
        "\n",
        "  '''#correlation heat map\n",
        "  create_corr_heat_map(df, nominal, ordinal, continuous, 0.9)\n",
        "\n",
        "  #dropping vars that are too related to another var based on corr analysis #todo!!\n",
        "  to_drop = []\n",
        "  df.drop(columns=[col for col in to_drop if col in df], inplace=True)\n",
        "\n",
        "  vars = [x for x in vars if x not in to_drop]\n",
        "  ordinal = [x for x in ordinal if x not in to_drop]\n",
        "  nominal = [x for x in nominal if x not in to_drop]\n",
        "  continuous = [x for x in continuous if x not in to_drop]'''; #todo:uncomment\n",
        "\n",
        "  return df, curr, vars, ordinal, nominal, continuous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG2UXB1A2wEf"
      },
      "outputs": [],
      "source": [
        "def request_vars(vars, base_request, limiters, target, num_vars, curr,\n",
        "                 ca_limit, nominal, ordinal, continuous):\n",
        "  #request data\n",
        "  request = base_request\n",
        "\n",
        "  #create request\n",
        "  while num_vars < 20 and curr < len(vars):\n",
        "    request += ','+vars[curr]\n",
        "    curr += 1\n",
        "    num_vars += 1\n",
        "\n",
        "  base_request = request\n",
        "\n",
        "  dfs = []\n",
        "\n",
        "  for i in range(len(limiters)):\n",
        "    l = limiters[i]\n",
        "    curr_request = request + l\n",
        "\n",
        "    if ca_limit:\n",
        "      curr_request += \"&ucgid=0400000US06\"\n",
        "\n",
        "    #get target/weight/ids\n",
        "    response = requests.get(curr_request)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    df = df.rename(columns=df.iloc[0]).loc[1:].reset_index(drop=True)\n",
        "\n",
        "    df[lim] = i\n",
        "    request = base_request\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "  new_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "  #there are no missing values, checked with print(new_df.isna().sum())\n",
        "\n",
        "  #drop identifiers\n",
        "  new_df.drop([housing_identifier, person_identifier, lim],axis=1,inplace=True)\n",
        "\n",
        "  #converting N (meaning N/A) to 0, have individually checked each var to confirm this is true\n",
        "  new_df.replace('N', 0, inplace = True)\n",
        "\n",
        "  #recode SOCP\n",
        "  if \"SOCP\" in new_df.columns:\n",
        "    new_df[\"SOCP\"] = recode(new_df, \"SOCP\")\n",
        "\n",
        "  #index\n",
        "  new_df = new_df.astype(int)\n",
        "  new_df = index(new_df)\n",
        "\n",
        "  #correlation heat map\n",
        "  create_corr_heat_map(new_df, nominal, ordinal, continuous, 0.9)\n",
        "\n",
        "  #dropping vars that are too related to another var based on corr analysis\n",
        "  to_drop = [\"DEVICE_ACC\", \"ELEP\", \"HHLDRAGEP\", \"INTERNET_ACC\", \"WATP\", \"HLTH_INSUR\",\n",
        "    \"RACNUM\", \"INSP\", \"NPF\", \"RNTP\", \"DRIVESP\", \"JWMNP\",\n",
        "    \"JWAP\", \"GRPIP\", \"MV\", \"MLPJ\", \"PLMPRP\", \"POWPUMA\", \"HOTWAT\",\n",
        "    \"GASFP\", \"MARHT\", \"MARHYP\", \"YRBLT\", \"DRAT\", \"HISP\",\n",
        "    \"WIF\", \"WKL\", \"VEH\", \"QTRBIR\", \"ACRR\", \"SCHG\",\n",
        "    \"SVAL\", \"AGS\", \"ENG\", \"HHT\", \"COW\", \"IMMIG\",\n",
        "    \"JWRIP\", \"JWDP\"]\n",
        "  new_df.drop(columns=[col for col in to_drop if col in new_df], inplace=True)\n",
        "\n",
        "  vars = [x for x in vars if x not in to_drop]\n",
        "  ordinal = [x for x in ordinal if x not in to_drop]\n",
        "  nominal = [x for x in nominal if x not in to_drop]\n",
        "  continuous = [x for x in continuous if x not in to_drop]\n",
        "\n",
        "  return new_df, curr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper functions"
      ],
      "metadata": {
        "id": "0rTm7md6vQdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pgxu72DPJfo"
      },
      "outputs": [],
      "source": [
        "def recode(df, col):\n",
        "  '''\n",
        "  returns recoded col\n",
        "  '''\n",
        "  unique = dict(enumerate(df[col].unique()))\n",
        "  unique = dict([(value, key) for key, value in unique.items()])\n",
        "  return df[col].replace(unique)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### correlation"
      ],
      "metadata": {
        "id": "bm3uAZrSvSij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corr_heat_map(df, nominal, ordinal, continuous, threshold):\n",
        "  '''\n",
        "  creates correlation heat map for the given df\n",
        "  '''\n",
        "  c = [col for col in continuous if col in df.columns]\n",
        "  o = [col for col in ordinal if col in df.columns]\n",
        "  n = [col for col in nominal if col in df.columns]\n",
        "\n",
        "  correlations = []\n",
        "\n",
        "  if c and o:\n",
        "    co_corr = nom.associations(df[c + o], nominal_columns = o,\n",
        "                               num_num_assoc = weighted_pearson,\n",
        "                               nom_num_assoc = weighted_spearman,\n",
        "                               nom_nom_assoc = weighted_spearman,\n",
        "                               compute_only = True)\n",
        "    correlations.append(co_corr[\"corr\"])\n",
        "  if c and n:\n",
        "    cn_corr = nom.associations(df[c + n], nominal_columns = n,\n",
        "                               num_num_assoc = weighted_pearson,\n",
        "                               nom_num_assoc = weighted_correlation_ratio,\n",
        "                               nom_nom_assoc = weighted_cramers_v,\n",
        "                               compute_only = True)\n",
        "    correlations.append(cn_corr[\"corr\"])\n",
        "  if o and n:\n",
        "    on_corr = nom.associations(df[n+o], nominal_columns = n,\n",
        "                               num_num_assoc = weighted_spearman,\n",
        "                               nom_num_assoc = weighted_cramers_v,\n",
        "                               nom_nom_assoc = weighted_cramers_v,\n",
        "                               compute_only = True)\n",
        "    correlations.append(on_corr[\"corr\"])\n",
        "\n",
        "  if not correlations:\n",
        "      return\n",
        "\n",
        "  #combine the dfs in correlations\n",
        "  all_labels = list(set(n + c + o))\n",
        "  corr = pd.DataFrame(index=all_labels, columns=all_labels)\n",
        "\n",
        "  for df in correlations:\n",
        "    corr.update(df)\n",
        "\n",
        "  corr = corr.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "  #create a heat map\n",
        "  rounded_cols = math.ceil(len(df.columns) / 21) * 21\n",
        "  plt.figure(figsize=(rounded_cols*0.7,rounded_cols*0.6))\n",
        "  sns.heatmap(corr, annot=True)\n",
        "  plt.show()\n",
        "\n",
        "  #pairs above threshold\n",
        "  for col in corr.columns:\n",
        "    for idx in corr.index:\n",
        "      if col != idx:\n",
        "        value = corr.loc[idx, col]\n",
        "        if abs(value) >= threshold:\n",
        "            print(f'{idx}, {col}: {value}')"
      ],
      "metadata": {
        "id": "R6SWzEXPDdcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLIGhJBBTR4M"
      },
      "outputs": [],
      "source": [
        "def weighted_mean(x, w):\n",
        "  '''weighted mean'''\n",
        "  return np.sum(x.astype(int) * w) / np.sum(w)\n",
        "\n",
        "def weighted_cov(x, y, w):\n",
        "  '''weighted covariance'''\n",
        "  return np.sum(w * (x.astype(int) - int(weighted_mean(x, w))) * (y - weighted_mean(y, w))) / np.sum(w)\n",
        "\n",
        "def weighted_pearson(x, y):\n",
        "  '''weighted pearson correlation; for continuous-continuous'''\n",
        "  #standardize\n",
        "  x = (x - x.mean()) / x.std()\n",
        "  y = (y - y.mean()) / y.std()\n",
        "\n",
        "  cov_xy = weighted_cov(x, y, weight_col)\n",
        "  cov_xx = weighted_cov(x, x, weight_col)\n",
        "  cov_yy = weighted_cov(y, y, weight_col)\n",
        "\n",
        "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
        "\n",
        "def weighted_spearman(x, y):\n",
        "  '''weighted spearman's rank correlation; for continuous-ordinal + ordinal-ordinal'''\n",
        "  rx = rankdata(x)\n",
        "  ry = rankdata(y)\n",
        "\n",
        "  d = (rx - ry)\n",
        "  w_sum_d_squared = np.sum(weight_col * d**2)\n",
        "  w_sum_d_rank_squared = np.sum(weight_col * rx)**2\n",
        "\n",
        "  n = len(x)\n",
        "  return 1 - (6 * w_sum_d_squared) / (n * (n**2 - 1) * w_sum_d_rank_squared)\n",
        "\n",
        "def weighted_cramers_v(x, y):\n",
        "  '''weighted cramer's v; for categorical data; for ordinal-nominal & nominal-nominal'''\n",
        "\n",
        "  contingency_table = pd.crosstab(x, y, values=weight_col, aggfunc=np.sum, normalize=False).fillna(0)\n",
        "\n",
        "  chi2, _, _, _ = chi2_contingency(contingency_table)\n",
        "\n",
        "  total_weight = weight_col.sum()\n",
        "\n",
        "  R, C = contingency_table.shape\n",
        "  degrees_of_freedom = (R - 1) * (C - 1)\n",
        "\n",
        "  return np.sqrt(chi2 / (total_weight * min(R-1, C-1)))\n",
        "\n",
        "def weighted_correlation_ratio(x, y):\n",
        "  '''weighted correlation ratio; for nominal(x)-continuous(y)'''\n",
        "\n",
        "  df = pd.DataFrame({'x': x, 'y': y, 'weight': weight_col})\n",
        "\n",
        "  y_weighted_mean = weighted_mean(y, weight_col)\n",
        "  total_variance = (weight_col * (y - y_weighted_mean) ** 2).sum()\n",
        "\n",
        "  df['weighted_mean'] = df.groupby('x')['y'].transform(lambda x: np.sum(x * df.loc[x.index, 'weight']) / np.sum(df.loc[x.index, 'weight']))\n",
        "  df['squared_weighted_diff'] = (df['y'] - df['weighted_mean'])**2 * df['weight']\n",
        "  between_group_variance = df['squared_weighted_diff'].sum()\n",
        "\n",
        "  result = between_group_variance / total_variance\n",
        "\n",
        "  return result\n",
        "\n",
        "def target_corr(df, target, weight, measure):\n",
        "  '''\n",
        "  returns new df with index as col names and value as relationship with target\n",
        "  using the given weighted measure (a callable function)\n",
        "  '''\n",
        "  cols = df.columns\n",
        "  cols = cols.drop([target, weight])\n",
        "\n",
        "  l = len(cols)-1\n",
        "\n",
        "  results = []\n",
        "  for col in cols:\n",
        "    results.append(measure(df[col], df[target]))\n",
        "\n",
        "  results = pd.DataFrame(results, index = cols, columns=[\"corr\"])\n",
        "\n",
        "  results = results[\"corr\"].sort_values(ascending=False)[1:]\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### indexing"
      ],
      "metadata": {
        "id": "53xGkXbJvWq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def index(df):\n",
        "  '''\n",
        "  indexes specific vars and replaces the original vars with the indexes.\n",
        "  prints error message if not all rows properly recoded.\n",
        "\n",
        "  precondition: as is true in 2021, census microdata doesn't include puerto rico\n",
        "  '''\n",
        "  cols = df.columns\n",
        "\n",
        "  cols_to_drop = []\n",
        "\n",
        "  if \"NATIVITY\" in cols:\n",
        "    df = index_immig(df)\n",
        "    cols_to_drop.extend([\"NATIVITY\", \"NOP\", \"CIT\", \"CITWP\"])\n",
        "\n",
        "  if \"ESP\" in cols:\n",
        "    df = index_parent_emp(df)\n",
        "    cols_to_drop.extend([\"ESP\"])\n",
        "\n",
        "  if \"HINS1\" in cols:\n",
        "    df = index_hlth_insur(df)\n",
        "    cols_to_drop.extend([\"HINS1\", \"HINS2\", \"HINS3\", \"HINS4\", \"HINS5\", \"HINS6\", \"HINS7\",\n",
        "                          \"HICOV\", \"PRIVCOV\", \"PUBCOV\", \"FHINS4C\", \"FHINS3C\", \"FHINS5C\"])\n",
        "\n",
        "  if \"OTHSVCEX\" in cols:\n",
        "    df = index_internet_acc(df)\n",
        "    cols_to_drop.extend([\"OTHSVCEX\", \"SATELLITE\", \"HISPEED\", \"ACCESSINET\", \"DIALUP\"])\n",
        "\n",
        "  if \"BROADBND\" in cols:\n",
        "    df = index_device_acc(df)\n",
        "    cols_to_drop.extend([\"BROADBND\", \"SMARTPHONE\", \"TEL\", \"TABLET\", \"LAPTOP\", \"COMPOTHX\"])\n",
        "\n",
        "  df = df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "WYhIYc-MG1po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_immig(df):\n",
        "  '''\n",
        "  1 -- not a citizen (cit = 5)\n",
        "  2 -- naturalized as adult (cit = 4, citwp = 18+)\n",
        "  3 -- teen-immigrant (cit = 4, citwp = 13+, under 18)\n",
        "  4 -- child-immigrant (cit = 4, citwp = 1+, under 13) -- the census nullifies migration records for those under 1\n",
        "  5 -- naturalized under 1 (cit = 4, citwp = under 1)\n",
        "  6 -- second-generation kid: nop = 4, 6, 8\n",
        "  7 - living with a native parent but has an immigrant parent (for kids): nop = 5, 7\n",
        "  8 -- living with an immigrant-parent and a native one (for kids): nop = 2, 3\n",
        "  9 - 3rd generation+ (neither parents are immigrants) (for kids): nop = 1\n",
        "  10 -- born abroad to u.s. citizen parent(s) (cit = 3)\n",
        "  11 - native born \"adult\" (greater than 17 years old/not an own child of\n",
        "  householder, and not child in subfamily) (cit = 1 or 2)\n",
        "\n",
        "  preconditions: (1) all nativity=1 have CIT = 1,2,3 and nativity=2 have CIT = 4,5\n",
        "                 (2) CITWP - (2021 - age) is typically at least 0 (all vals under 0\n",
        "                  are close, -3 is the max value + they're all for naturalizations\n",
        "                  very young in the 40s/50s)\n",
        "                 (3) all 17+ with ESP = 0 (not own child of householder and not\n",
        "                  child in subfamily) have NOP = 0 (N/A)\n",
        "  '''\n",
        "  col = 'IMMIG'\n",
        "  df.loc[df['CIT'] == 5, col] = 1\n",
        "  df.loc[(df['CIT'] == 4) & (df['CITWP'].astype(int) - 2021 + df['AGEP'].astype(int) < 13), col] = 4\n",
        "  df.loc[(df['CIT'] == 4) & (df['CITWP'].astype(int) - 2021 + df['AGEP'].astype(int) < 18), col] = 3\n",
        "  df.loc[df['CIT'] == 4, col] = 2\n",
        "  df.loc[(df['CIT'] == 4) & (df['CITWP'].astype(int) - 2021 + df['AGEP'].astype(int) < 1), col] = 5\n",
        "  df.loc[df['NOP'].isin([4, 6, 8]), col] = 6\n",
        "  df.loc[df['NOP'].isin([5, 7]), col] = 7\n",
        "  df.loc[df['NOP'].isin([2, 3]), col] = 8\n",
        "  df.loc[df['NOP'] == 1, col] = 9\n",
        "  df.loc[df['CIT'] == 3, col] = 10\n",
        "\n",
        "  '''note: there are distinctions between the experiences of those who are\n",
        "  17+/not an own child of householder/not child in subfamily born here who are\n",
        "  the children of immigrants vs those who are not, but that is a distinction\n",
        "  the census does not make and thus the recoded variable cannot make that distinction'''\n",
        "  df.loc[df['CIT'].isin([1, 2]), col] = 11\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def index_parent_emp(df):\n",
        "  '''\n",
        "  esp: employment status of parents\n",
        "  0 (0) -- not own child of householder, and not child in subfamily\n",
        "  1 (1) -- 2 parents, both in labor force\n",
        "  2 (2,3) - 2 parents, one in labor force\n",
        "  3 (5, 7) -- single parent, in labor force\n",
        "  4 (6, 8) -- single parent, not in labor force\n",
        "  5 (4) - 2 parents, neither in labor force\n",
        "  '''\n",
        "  col = \"PARENT_EMP\"\n",
        "  df.loc[df['ESP'] == 0, col] = 0\n",
        "  df.loc[df['ESP'] == 1, col] = 1\n",
        "  df.loc[df['ESP'].isin([2, 3]), col] = 2\n",
        "  df.loc[df['ESP'].isin([5, 7]), col] = 3\n",
        "  df.loc[df['ESP'].isin([6, 8]), col] = 4\n",
        "  df.loc[df['ESP'] == 4, col] = 5\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def index_hlth_insur(df):\n",
        "  '''\n",
        "  0 - no health insurance (HICOV = 2, HINS7 = 2)\n",
        "  1 - indian health service (HINS7 = 1)\n",
        "  2 - public health insurance (PUBCOV = 1)\n",
        "  3 - private health insurance (PRIVCOV = 1)\n",
        "  census health insurance recode documentation: https://www.census.gov/topics/health/health-insurance/guidance/programming-code/acs-recoding.html\n",
        "  '''\n",
        "  col = \"HLTH_INSUR\"\n",
        "  df.loc[(df['HICOV'] == 2) & (df['HINS7'] == 2), col] = 0\n",
        "  df.loc[df['HINS7'] == 1, col] = 1\n",
        "  df.loc[df['PUBCOV'] == 1, col] = 2\n",
        "  df.loc[df['PRIVCOV'] == 1, col] = 3\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def index_internet_acc(df):\n",
        "  '''\n",
        "  0 - n/a (group quarters (e.g. dorm) or vacant)\n",
        "  1 - no internet (ACCESSINET = 3)\n",
        "  2 - unpaid internet (ACCESSINET = 2)\n",
        "  3 - dial up (DIALUP = 1)\n",
        "  4 - internet thru cellphone company (ACCESSINET = 1 & others = 2)\n",
        "  5 - satellite/other (SATELLITE = 1 or OTHSVCEX = 1)\n",
        "  6 - hispeed (HISPEED = 1)\n",
        "  '''\n",
        "  col = \"INTERNET_ACC\"\n",
        "  df.loc[(df['OTHSVCEX'] == 0) & (df['SATELLITE'] == 0) &\n",
        "              (df['HISPEED'] == 0) & (df['ACCESSINET'] == 0), col] = 0\n",
        "  df.loc[df['ACCESSINET'] == 3, col] = 1\n",
        "  df.loc[df['ACCESSINET'] == 2, col] = 2\n",
        "  df.loc[(df['DIALUP'] == 1), col] = 3\n",
        "  df.loc[(df['OTHSVCEX'] == 2) & (df['SATELLITE'] == 2) &\n",
        "              ((df['HISPEED'] == 2) & (df['ACCESSINET'] == 1)), col] = 4\n",
        "  df.loc[(df['SATELLITE'] == 1) | (df['OTHSVCEX'] == 1), col] = 5\n",
        "  df.loc[df['HISPEED'] == 1, col] = 6\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def index_device_acc(df):\n",
        "  '''\n",
        "  0 - n/a (group quarters (e.g. dorm) or vacant)\n",
        "  1 - no devices (TEL = 2, SMARTPHONE = 2, TABLET = 2, LAPTOP = 2)\n",
        "  2 - telephone access\n",
        "  3 - laptop or tablet\n",
        "  4 - smartphone access\n",
        "  5 - smartphone access + laptop or tablet\n",
        "  6 - smartphone with data (BROADBND = 1, SMARTPHONE = 1)\n",
        "  7 - smartphone with data + laptop or tablet\n",
        "\n",
        "  note: tried recoding taking the value of COMPOTHX into account, and it didn't\n",
        "  make a difference\n",
        "  '''\n",
        "  col = \"DEVICE_ACC\"\n",
        "\n",
        "  computer = ((df['TABLET'] == 1) | (df['LAPTOP'] == 1))\n",
        "\n",
        "  df.loc[(df['SMARTPHONE'] == 1) & (df['BROADBND'] == 1) &\n",
        "              computer, col] = 7\n",
        "  df.loc[(df['SMARTPHONE'] == 1) & (df['BROADBND'] == 1), col] = 6\n",
        "  df.loc[(df['SMARTPHONE'] == 1) & computer, col] = 5\n",
        "  df.loc[(df['SMARTPHONE'] == 1), col] = 4\n",
        "  df.loc[computer, col] = 3\n",
        "  df.loc[df['TEL'] == 1, col] = 2\n",
        "  df.loc[(df['TEL'] == 2) & (df['SMARTPHONE'] == 2) &\n",
        "          (df['TABLET'] == 2) & (df['LAPTOP'] == 2), col] = 1\n",
        "\n",
        "\n",
        "  df.loc[(df['SMARTPHONE'] == 0) & (df['BROADBND'] == 0) &\n",
        "              ~computer, col] = 0\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "zMisoiUlbUHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lasso"
      ],
      "metadata": {
        "id": "Q7w0JOxNY2lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lasso(data, target): #todo:edit\n",
        "    x = data.drop([target, housing_identifier, person_identifier], axis=1)\n",
        "    x_weight = x[weight]\n",
        "    x = x.drop(weight, axis=1)\n",
        "    y = data[target]\n",
        "\n",
        "    #standardize data\n",
        "    scaler = StandardScaler()\n",
        "    x_normalized = scaler.fit_transform(x)\n",
        "\n",
        "    #split into test & train\n",
        "    x_train, x_test, y_train, y_test, train_weights, test_weights = train_test_split(x_normalized, y, x_weight, test_size=0.2, random_state=42)\n",
        "    print(train_weights)\n",
        "\n",
        "    #train lasso\n",
        "    model = Lasso(alpha=0.9)  # todo: adjust alpha value\n",
        "    model.fit(x_train, y_train, sample_weight=train_weights)\n",
        "\n",
        "    #coefficients\n",
        "    coefficients = pd.Series(model.coef_, index=x.columns)\n",
        "    selected_features = coefficients[coefficients != 0]\n",
        "\n",
        "    #evaluate model's performance\n",
        "    y_pred = model.predict(x_test)\n",
        "    mse = mean_squared_error(y_test, y_pred, sample_weight=test_weights)\n",
        "    print(\"mse:\", mse)\n",
        "\n",
        "    return selected_features"
      ],
      "metadata": {
        "id": "d5-8jt5vebcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# analysis"
      ],
      "metadata": {
        "id": "ImLD7vZ8uEAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unTo2cmq29aB"
      },
      "outputs": [],
      "source": [
        "limiters = [\"&RACPI=1&RACASN=1\", \"&RACPI=0&RACASN=1\", \"&RACPI=1&RACASN=0\"] #all aapi\n",
        "drop = [\"RACPI\", \"RACASN\"]\n",
        "#limiters = [\"&RACPI=1&RACASN=1\", \"&RACPI=1&RACASN=0\"] #aa/pi mixed + pi\n",
        "data, vars, nominal, ordinal, continuous = import_census_data(vars, \"PINCP\", \"2021\", limiters, False, drop, nominal, ordinal, continuous)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = lasso(data, \"PINCP\")\n",
        "print(features)"
      ],
      "metadata": {
        "id": "3QCyRq_sadpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def request_data(request, limiters=[]):\n",
        "  dfs = []\n",
        "\n",
        "  if limiters == []: #if no limiters\n",
        "    response = requests.get(request)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    return df.rename(columns=df.iloc[0]).loc[1:].reset_index()\n",
        "\n",
        "  for l in limiters:\n",
        "    response = requests.get(request+l)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    df = df.rename(columns=df.iloc[0]).loc[1:].reset_index()\n",
        "    dfs.append(df)\n",
        "\n",
        "  return pd.concat(dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "LewCoCCI7f7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "native = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,SERIALNO,SPORDER,CIT&NATIVITY=1\")\n",
        "immigrants = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,SERIALNO,SPORDER,CIT,AGEP,CITWP&NATIVITY=2\")\n",
        "\n",
        "#checked all native-born have CIT = 1, 2, or 3 + all foreign-born have CIT = 4,5\n",
        "print(native['CIT'].unique())\n",
        "print(immigrants['CIT'].unique())"
      ],
      "metadata": {
        "id": "WQtXi5GIC6pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all naturalized were naturalized on or after their approx birth year except\n",
        "#for 19 people naturalized in the 50s when very young\n",
        "naturalized = immigrants[immigrants['CIT'] == \"4\"]\n",
        "correct_years = (2021 - naturalized['AGEP'].astype(int)) <= naturalized['CITWP'].astype(int)+1\n",
        "naturalized['diff'] = naturalized['CITWP'].astype(int) - 2021 + naturalized['AGEP'].astype(int)\n",
        "print(naturalized[~correct_years])"
      ],
      "metadata": {
        "id": "Xqi4OjTSZhhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checked all 17+ with ESP = 0 (not own child of householder and not child in subfamily) have NOP = 0\n",
        "no_parents = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,SERIALNO,SPORDER,NOP,ESP,AGEP\", [\"&AGEP=18:99\", \"&ESP=0\"])\n",
        "print((no_parents['NOP'] == 0).all())"
      ],
      "metadata": {
        "id": "O7bGnciLaMoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_unique_vals(df, vars):\n",
        "  for col in vars:\n",
        "    print(\"col\")\n",
        "    print(df[col].value_counts())"
      ],
      "metadata": {
        "id": "19h4ewX18j-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#understanding how race vars are determined for someone who is chinese & indian\n",
        "df = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,\" +\n",
        "                  \"SERIALNO,SPORDER,RAC1P,RAC2P,RAC3P,RACBLK\",\n",
        "                   [\"&ANC1P=706&ANC2P=615\", \"&ANC1P=615&ANC2P=706\"])\n",
        "print(df)\n",
        "print_unique_vals(df, [\"RAC1P\", \"RAC2P\", \"RAC3P\", \"RACBLK\"])\n",
        "print(df[df[\"RACBLK\"]==\"1\"])\n",
        "print_unique_vals(df[df[\"RACBLK\"]==\"1\"], [\"RAC1P\", \"RAC2P\", \"RAC3P\"])"
      ],
      "metadata": {
        "id": "pEe5g0gg2Ssm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looking at \"mixture\" ancestry -- all other ancestry values are single ethnicity\n",
        "df = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,\" +\n",
        "                  \"SERIALNO,SPORDER,RAC1P,RAC2P,RAC3P\",\n",
        "                   [\"&ANC1P=995\", \"&ANC2P=995\"])\n",
        "print_unique_vals(df, [\"RAC1P\", \"RAC2P\", \"RAC3P\"])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "MRHkXsVQ7YZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBbdpmEAAki3"
      },
      "outputs": [],
      "source": [
        "#checking relationship between specific RAC3P (ethnicity) values + income\n",
        "df = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,PINCP,SERIALNO,SPORDER&RAC3P=004&RAC3P=006&RAC3P=013\")\n",
        "df.drop([housing_identifier, person_identifier],axis=1,inplace=True)\n",
        "df = df.astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxxs790rBUU7"
      },
      "outputs": [],
      "source": [
        "#histogram\n",
        "unique = df['RAC3P'].unique()\n",
        "ethnicities = {6:\"filipino\",4:\"indian\",13:\"samoan\"}\n",
        "\n",
        "for val in unique:\n",
        "  new_df = df[df[\"RAC3P\"] == val]\n",
        "  plt.hist(new_df[\"PINCP\"], weights=new_df[\"PWGTP\"], label=ethnicities[val],alpha=.2)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modeling"
      ],
      "metadata": {
        "id": "rqEg00Z9Z8_O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}