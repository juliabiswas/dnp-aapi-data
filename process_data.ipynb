{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIlwbX55NlAo"
      },
      "source": [
        "# libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYWOXvW-U7rF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C9XiNqiibIG"
      },
      "outputs": [],
      "source": [
        "!pip install dython\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from scipy.stats import rankdata\n",
        "from scipy.stats import chi2_contingency\n",
        "from dython import nominal as nom\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqqXz5sKt7x3"
      },
      "source": [
        "# data processing\n",
        "\n",
        "data: the census bureau’s 2021 american community survey (acs) public use microdata sample (pums) \\\\\n",
        "data dictionary: https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2021.pdf \\\\\n",
        "data dictionary for final dataset: https://docs.google.com/document/d/1hGPmwgD06HIE2_xxVq_vM5hiXBkpkHPKOc7wLQHmA8M/edit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoKfe2h2icXU"
      },
      "outputs": [],
      "source": [
        "target = \"PINCP\"\n",
        "weight = \"PWGTP\"\n",
        "housing_identifier = \"SERIALNO\"\n",
        "person_identifier = \"SPORDER\"\n",
        "lim = \"limiter\"\n",
        "\n",
        "weight_col = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7WlIqwW242p"
      },
      "outputs": [],
      "source": [
        "vars = [\"PINCP\", \"PWGTP\", \"SERIALNO\", \"SPORDER\", \"HHLDRAGEP\", \"BROADBND\", \"SMARTPHONE\", \"TEL\",\n",
        "        \"TABLET\", \"LAPTOP\", \"ACCESSINET\", \"COMPOTHX\", \"OTHSVCEX\", \"SATELLITE\", \"HISPEED\",\n",
        "        \"DIALUP\", \"RAC2P\", \"RAC3P\", \"RAC1P\", \"ANC2P\", \"ANC1P\", \"HINS1\", \"HINS2\", \"HINS3\",\n",
        "        \"HINS4\", \"HINS5\", \"HINS6\", \"HINS7\", \"HICOV\",\n",
        "        \"PRIVCOV\", \"PUBCOV\", \"FHINS4C\", \"FHINS3C\", \"FHINS5C\",\n",
        "        \"SSIP\", \"ELEP\", \"RACNUM\", \"WATP\", \"MHP\", \"RETP\",\n",
        "        \"SSP\", \"HINCP\", \"RMSP\", \"INTP\", \"SEMP\",\n",
        "        \"SMP\", \"PERNP\", \"PAP\", \"GASP\", \"WKWN\", \"WAGP\", \"FULP\", \"SMOCP\",\n",
        "        \"FINCP\", \"OIP\", \"TAXAMT\", \"CONP\", \"INSP\", \"OCPIP\", \"GRNTP\", \"MRGP\",\n",
        "        \"VALP\", \"BDSP\", \"NOC\", \"NP\", \"NRC\", \"NPF\", \"RNTP\",\n",
        "        \"WKHP\", \"POVPIP\", \"GRPIP\", \"JWMNP\", \"ADJHSG\", \"ADJINC\", \"MV\",\n",
        "        \"FPARC\", \"DRIVESP\", \"RACSOR\", \"JWAP\", \"R60\", \"RELSHIPP\", \"VACDUR\",\n",
        "        \"MLPIK\", \"PLM\", \"VPS\", \"DEAR\", \"R18\", \"MLPJ\", \"GCL\", \"STOV\",\n",
        "        \"ELEFP\", \"WATFP\", \"YOEP\", \"SMX\", \"MLPCD\",\n",
        "        \"WRK\", \"POBP\", \"RACAIAN\", \"HHT2\", \"MLPFG\", \"FOD1P\",\n",
        "        \"FOD2P\", \"NAICSP\", \"INDP\", \"WAOB\", \"SOCP\", \"GASFP\",\n",
        "        \"HIMRKS\", \"HOTWAT\", \"NWLA\", \"JWTRNS\", \"REFR\", \"PSF\",\n",
        "        \"DECADE\", \"FULFP\", \"MRGT\", \"VACOTH\", \"LANP\",\n",
        "        \"TEN\", \"POWPUMA\", \"PLMPRP\", \"CPLT\", \"YRBLT\", \"DRAT\",\n",
        "        \"NR\", \"MRGX\", \"MARHYP\", \"SINK\", \"MARHT\",\n",
        "        \"WIF\", \"HISP\", \"MAR\", \"SCHL\", \"NWLK\", \"DPHY\", \"DEYE\", \"MIGSP\",\n",
        "        \"HHLANP\", \"PARTNER\", \"RACNH\", \"WKL\", \"VEH\", \"DDRS\", \"MIGPUMA\",\n",
        "        \"LNGI\", \"QTRBIR\", \"SFN\", \"RACBLK\", \"MLPH\", \"ESR\", \"NPP\", \"DIS\",\n",
        "        \"HHLDRRAC1P\", \"MLPB\", \"DOUT\", \"SCH\", \"RACPI\",\n",
        "        \"POWSP\", \"ANC\", \"MIL\", \"OC\", \"HUGCL\", \"RWAT\", \"HHLDRHISP\", \"RESMODE\",\n",
        "        \"MARHW\", \"SFR\", \"ESP\", \"RACASN\", \"MLPE\", \"OCCP\", \"MARHD\", \"SCHG\",\n",
        "        \"MRGI\", \"MIG\", \"MSP\", \"FER\", \"MULTG\", \"WORKSTAT\", \"MARHM\", \"KIT\",\n",
        "        \"GCR\", \"HUPARC\", \"GCM\", \"ACR\", \"PAOC\", \"RNTM\", \"DRATX\", \"FS\",\n",
        "        \"SVAL\", \"RACWHT\", \"NWAB\", \"HUPAOC\", \"R65\", \"RC\", \"BATH\", \"SEX\",\n",
        "        \"HFL\", \"WKEXREL\", \"VACS\", \"HHL\", \"SRNT\", \"NWAV\", \"NWRE\", \"BLD\",\n",
        "        \"LANX\", \"MLPA\", \"HHT\", \"DREM\", \"COW\", \"HUPAC\", \"NATIVITY\", \"NOP\", \"AGEP\", \"CITWP\", \"CIT\",\n",
        "        \"AGS\", \"ENG\", \"JWRIP\", \"JWDP\"]\n",
        "ordinal = [\"PARENT_EMP\", \"HLTH_INSUR\", \"INTERNET_ACC\", \"DEVICE_ACC\",\n",
        "           \"QTRBIR\", \"R60\", \"ACR\", \"AGS\", \"BLD\", \"ENG\", \"GCM\", \"LNGI\",\n",
        "           \"WIF\", \"WKL\", \"MARHT\", \"SCH\", \"SCHG\", \"SCHL\", \"DRAT\", \"DECADE\",\n",
        "           \"YOEP\", \"MARHYP\", \"VACDUR\", \"YRBLT\", \"MV\", \"JWAP\", \"VEH\", \"JWRIP\",\n",
        "           \"JWDP\", \"AGEP\", \"HHLDRAGEP\", \"CITWP\"]\n",
        "nominal = [\"FPARC\", \"COW\", \"DDRS\", \"DPHY\", \"DREM\", \"ESR\", \"GCL\", \"HFL\", \"HHL\",\n",
        "           \"HHT\", \"HUGCL\", \"HUPAC\", \"HUPAOC\", \"HUPARC\", \"LANX\", \"MAR\", \"MLPA\",\n",
        "           \"MLPB\", \"MLPE\", \"MLPH\", \"MLPJ\",\n",
        "          \"MSP\", \"NPP\", \"NR\", \"OC\", \"PAOC\", \"PSF\", \"R18\",\n",
        "          \"R65\", \"RC\", \"RNTM\", \"SEX\", \"SFN\", \"SFR\",\n",
        "          \"SRNT\", \"SVAL\", \"VACS\", \"WKEXREL\", \"WORKSTAT\", \"PARTNER\",\n",
        "          \"VPS\", \"GCR\", \"FS\", \"INSP\", \"DRATX\", \"DOUT\", \"DIS\",\n",
        "          \"DEYE\", \"DEAR\", \"BATH\", \"KIT\", \"MARHM\", \"MARHW\",\n",
        "          \"MIG\", \"MRGI\", \"MRGT\", \"MRGX\", \"MULTG\", \"REFR\", \"RWAT\",\n",
        "          \"SINK\", \"SMX\", \"STOV\", \"TEN\", \"WRK\", \"PLM\", \"FER\",\n",
        "          \"MIL\", \"RESMODE\", \"MLPCD\", \"MLPFG\", \"PLMPRP\", \"HOTWAT\",\n",
        "          \"WAOB\", \"ELEFP\", \"FULFP\", \"GSFP\", \"WATFP\", \"CPLT\",\n",
        "          \"HHT2\", \"HIMRKS\", \"JWTRNS\", \"RELSHIPP\", \"ANC\", \"ANC1P\",\n",
        "          \"ANC2P\", \"FOD1P\", \"FOD2P\", \"HHLANP\", \"INDP\", \"LANP\",\n",
        "          \"MARHD\", \"HHLDRHISP\", \"HHLDRRAC1P\", \"MIGPUMA\", \"MIGSP\",\n",
        "          \"MLPIK\", \"OCCP\", \"POBP\", \"PWPUMA\", \"POWSP\", \"SOCP\",\n",
        "          \"VACOTH\", \"RAC2P\", \"RAC3P\", \"RAC1P\", \"HISP\", \"POWPUMA\",\n",
        "           \"NATIVITY\", \"NOP\", \"CIT\", \"RACAIAN\",  \"RACWHT\", \"RACBLK\",\n",
        "           \"RACSOR\"]\n",
        "continuous = [\"SSIP\", \"ELEP\", \"RACNUM\", \"WATP\", \"MHP\", \"RETP\", \"SSP\", \"HINCP\",\n",
        "              \"RMSP\", \"INTP\", \"SEMP\",\n",
        "              \"SMP\", \"PERNP\", \"PAP\", \"GASP\", \"WKWN\",\n",
        "              \"WAGP\", \"FULP\", \"SMOCP\", \"FINCP\", \"OIP\",\n",
        "              \"TAXAMT\", \"CONP\", \"OCPIP\", \"GRNTP\", \"MRGP\",\n",
        "              \"VALP\", \"BDSP\", \"NOC\", \"NP\", \"NRC\",\n",
        "              \"NPF\", \"RNTP\", \"WKHP\", \"POVPIP\", \"JWMNP\",\n",
        "              \"DRIVESP\", \"GASFP\", \"GRPIP\", \"PINCP\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSfUTcH8vNpS"
      },
      "source": [
        "## main functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dha9-i0uxQ6q"
      },
      "outputs": [],
      "source": [
        "def import_census_data(variables, year, limiters, ca_limit, drop,\n",
        "                       ordinal, nominal, continuous):\n",
        "  '''\n",
        "  imports census data\n",
        "\n",
        "  variables: list of variables to import from the census\n",
        "  year: year of the census to import data from\n",
        "  limiters: any limiters on scope of the data\n",
        "  ca_limit: whether to limit data to just california\n",
        "  drop: variables to not include in the final dataset (i.e. variables involved\n",
        "    in limiters that might be redundant info)\n",
        "  ordinal: ordinal vars in \"variables\"\n",
        "  nominal: nominal vars in \"variables\"\n",
        "  continuous: continuous vars in \"variables\"\n",
        "\n",
        "  returns the imported dataset, post-indexing & correlation analysis\n",
        "  '''\n",
        "  vars = variables[:]\n",
        "\n",
        "  base_request = \"https://api.census.gov/data/\" + year + \"/acs/acs1/pums?get=\"\n",
        "  base_request += weight + \",\" + target\n",
        "  base_request += \",\" + housing_identifier + \",\" + person_identifier\n",
        "  request = base_request\n",
        "\n",
        "  #remove base vars\n",
        "  vars.remove(target)\n",
        "  vars.remove(weight)\n",
        "  vars.remove(housing_identifier)\n",
        "  vars.remove(person_identifier)\n",
        "\n",
        "  for v in drop:\n",
        "    vars.remove(v)\n",
        "\n",
        "  dfs = []\n",
        "\n",
        "  for i in range(len(limiters)):\n",
        "    l = limiters[i]\n",
        "    request += l\n",
        "\n",
        "    if ca_limit:\n",
        "      request += \"&ucgid=0400000US06\"\n",
        "\n",
        "    #get target/weight/ids\n",
        "    response = requests.get(request)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    df = df.rename(columns=df.iloc[0]).loc[1:].reset_index()\n",
        "\n",
        "    df[lim] = i\n",
        "    dfs.append(df)\n",
        "    request = base_request\n",
        "\n",
        "  data = pd.concat(dfs, ignore_index=True)\n",
        "  data = data.applymap(try_convert_to_int)\n",
        "\n",
        "  #weight col\n",
        "  global weight_col\n",
        "  weight_col = data[weight]\n",
        "\n",
        "  #dropping uneeded vars\n",
        "  to_drop = [\"ADJINC\", \"ADJHSG\"] #only one unique value so don't need\n",
        "  to_drop.extend([\"NWAB\", \"NWAV\", \"NWLA\", \"NWLK\", \"NWRE\"]) #ESR recoded them so can drop\n",
        "\n",
        "  '''naicsp gives the same info as indp (indp is derived from naicsp), and socp\n",
        "  gives the same as occp (occp is derived from socp). naicsp and socp are eliminated\n",
        "  because of this redundancy since the census recommends using occp and indp\n",
        "  to protect individual respondents participating in Census surveys or health surveys\n",
        "  (see more info here: https://www.cdc.gov/niosh/topics/coding/more.html)'''\n",
        "  to_drop.extend([\"NAICSP\", \"SOCP\"])\n",
        "\n",
        "  vars = [x for x in vars if x not in to_drop]\n",
        "  ordinal = [x for x in ordinal if x not in to_drop]\n",
        "  nominal = [x for x in nominal if x not in to_drop]\n",
        "  continuous = [x for x in continuous if x not in to_drop]\n",
        "\n",
        "  #requesting all data\n",
        "  curr = 0\n",
        "  while curr < len(vars):\n",
        "    data, curr, vars, ordinal, nominal, continuous = request_vars_and_merge(vars, data,\n",
        "                                                                            base_request,\n",
        "                                                                            limiters,\n",
        "                                                                            4, curr, ca_limit,\n",
        "                                                                            ordinal, nominal,\n",
        "                                                                            continuous, drop)\n",
        "\n",
        "  data.drop(columns=[lim, 'index']+drop, axis=1, inplace=True)\n",
        "  data.to_csv('/content/drive/My Drive/data.csv', index=False) #save to csv\n",
        "\n",
        "  return data, vars, ordinal, nominal, continuous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2CZsx5b07nS"
      },
      "outputs": [],
      "source": [
        "def request_vars_and_merge(vars, df, base_request, limiters, num_vars, curr,\n",
        "                           ca_limit, ordinal, nominal, continuous, drop):\n",
        "  '''\n",
        "  imports a chunk of census data and merges it with the existing dataframe\n",
        "  (the census api only allows a max of 50 variables to be requested at once)\n",
        "\n",
        "  vars: full list of variables to import from the census\n",
        "  df: existing dataframe\n",
        "  base_request: the base api request\n",
        "  limiters: any limiters on scope of the data\n",
        "  num_vars: number of variables in base_request\n",
        "  curr: current index in vars\n",
        "  ca_limit: whether to limit data to just california\n",
        "  ordinal: ordinal vars in \"variables\"\n",
        "  nominal: nominal vars in \"variables\"\n",
        "  continuous: continuous vars in \"variables\"\n",
        "  drop: variables to not include in the final dataset (i.e. variables involved\n",
        "    in limiters that might be redundant info)\n",
        "\n",
        "  returns the current dataframe with a new data chunk merged in, post-indexing\n",
        "  & correlation analysis\n",
        "  '''\n",
        "\n",
        "  #get new vars\n",
        "  new_df, curr, vars, ordinal, nominal, continuous = request_vars(vars, base_request, limiters, num_vars,\n",
        "                              curr, ca_limit, ordinal, nominal, continuous)\n",
        "  if curr < len(vars):\n",
        "    curr_element = vars[curr]\n",
        "  else:\n",
        "    curr_element = None\n",
        "\n",
        "  #merge with existing df\n",
        "  on_cols = [housing_identifier, person_identifier, weight, target, lim]\n",
        "  if ca_limit:\n",
        "    on_cols.append(\"ST\")\n",
        "  on_cols.extend(drop)\n",
        "  df = pd.merge(df, new_df, on=on_cols)\n",
        "  df.drop_duplicates(keep='first', inplace=True, ignore_index=True)\n",
        "\n",
        "  #correlation heat maps\n",
        "  df, vars, ordinal, nominal, continuous = create_corr_heat_map(df, vars, ordinal, nominal, continuous, 0.9, True)\n",
        "\n",
        "  #update curr\n",
        "  if curr_element is not None:\n",
        "    curr = vars.index(curr_element)\n",
        "  else:\n",
        "    curr = len(vars)\n",
        "\n",
        "  return df, curr, vars, ordinal, nominal, continuous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG2UXB1A2wEf"
      },
      "outputs": [],
      "source": [
        "def request_vars(vars, base_request, limiters, num_vars, curr,\n",
        "                 ca_limit, ordinal, nominal, continuous):\n",
        "  '''\n",
        "  imports a chunk of census data\n",
        "\n",
        "  vars: full list of variables to import from the census\n",
        "  base_request: the base api request\n",
        "  limiters: any limiters on scope of the data\n",
        "  num_vars: number of variables in base_request\n",
        "  curr: current index in vars\n",
        "  ca_limit: whether to limit data to just california\n",
        "  ordinal: ordinal vars in \"variables\"\n",
        "  nominal: nominal vars in \"variables\"\n",
        "  continuous: continuous vars in \"variables\"\n",
        "\n",
        "  returns the imported data chunk, post-indexing & correlation analysis\n",
        "  '''\n",
        "\n",
        "  #request data\n",
        "  request = base_request\n",
        "\n",
        "  #create request\n",
        "  while num_vars < 20 and curr < len(vars):\n",
        "    request += ','+vars[curr]\n",
        "    curr += 1\n",
        "    num_vars += 1\n",
        "\n",
        "  base_request = request\n",
        "  if curr < len(vars):\n",
        "    curr_element = vars[curr]\n",
        "  else:\n",
        "    curr_element = None\n",
        "\n",
        "  dfs = []\n",
        "\n",
        "  for i in range(len(limiters)):\n",
        "    l = limiters[i]\n",
        "    curr_request = request + l\n",
        "\n",
        "    if ca_limit:\n",
        "      curr_request += \"&ucgid=0400000US06\"\n",
        "\n",
        "    response = requests.get(curr_request)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    df = df.rename(columns=df.iloc[0]).loc[1:].reset_index(drop=True)\n",
        "\n",
        "    df[lim] = i\n",
        "    request = base_request\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "  new_df = pd.concat(dfs, ignore_index=True)\n",
        "  new_df = new_df.applymap(try_convert_to_int)\n",
        "\n",
        "  #there are no missing values, checked with print(new_df.isna().sum())\n",
        "\n",
        "  #drop identifiers\n",
        "  new_hi_col = new_df[housing_identifier]\n",
        "  new_pi_col = new_df[person_identifier]\n",
        "\n",
        "  new_df.drop([housing_identifier, person_identifier],axis=1,inplace=True)\n",
        "\n",
        "  #converting N (meaning N/A) to 0, have individually checked each var to confirm this is true\n",
        "  new_df.replace('N', 0, inplace = True)\n",
        "\n",
        "  #index\n",
        "  new_df, vars = index(new_df, vars)\n",
        "\n",
        "  #create ethnoracial vars\n",
        "  if \"RAC2P\" in new_df.columns:\n",
        "    new_df, nominal = create_ethnoracial_vars(new_df, nominal)\n",
        "\n",
        "  #correlation heat map\n",
        "  new_df, vars, ordinal, nominal, continuous = create_corr_heat_map(new_df, vars, ordinal, nominal, continuous, 0.9, True)\n",
        "\n",
        "  if curr_element is not None:\n",
        "    curr = vars.index(curr_element)\n",
        "  else:\n",
        "    curr = len(vars)\n",
        "\n",
        "  #reinstate identifiers\n",
        "  new_df.loc[:,housing_identifier] = new_hi_col\n",
        "  new_df.loc[:,person_identifier] = new_pi_col\n",
        "\n",
        "  return new_df, curr, vars, ordinal, nominal, continuous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rTm7md6vQdf"
      },
      "source": [
        "## helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pgxu72DPJfo"
      },
      "outputs": [],
      "source": [
        "def recode(df, col):\n",
        "  '''\n",
        "  returns recoded col\n",
        "  '''\n",
        "  unique = dict(enumerate(df[col].unique()))\n",
        "  unique = dict([(value, key) for key, value in unique.items()])\n",
        "  return df[col].replace(unique)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def try_convert_to_int(value):\n",
        "    '''convert to int if possible'''\n",
        "    try:\n",
        "      return int(value)\n",
        "    except ValueError:\n",
        "      return value"
      ],
      "metadata": {
        "id": "ERgdVNtp_WLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm3uAZrSvSij"
      },
      "source": [
        "### correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6SWzEXPDdcj"
      },
      "outputs": [],
      "source": [
        "def create_corr_heat_map(df, vars, ordinal, nominal, continuous, threshold, dropping):\n",
        "  '''\n",
        "  correlation analysis to avoid multicollinearity: includes creation of a heat\n",
        "  map to visualize correlations\n",
        "\n",
        "  df: dataframe with data\n",
        "  vars: full list of variables to import from the census\n",
        "  ordinal: ordinal vars in \"variables\"\n",
        "  nominal: nominal vars in \"variables\"\n",
        "  continuous: continuous vars in \"variables\"\n",
        "  threshold: threshold by which to check variable pairwise comparisons\n",
        "    (>= threshold eliminated)\n",
        "  dropping: whether or not to drop variables in correlation analysis;\n",
        "    if False, just creates heatmap & threshold value doesn't matter\n",
        "\n",
        "  precondition: all values in df cols that are in ordinal, nominal, and continuous\n",
        "  must be numeric. target must be in df if dropping = True.\n",
        "  '''\n",
        "  df = df.applymap(try_convert_to_int)\n",
        "\n",
        "  o = [col for col in ordinal if col in df.columns]\n",
        "  n = [col for col in nominal if col in df.columns]\n",
        "  c = [col for col in continuous if col in df.columns]\n",
        "\n",
        "  correlations = []\n",
        "\n",
        "  if len(c) + len(o) >= 2:\n",
        "    correlations.append(nom.associations(df[c + o], nominal_columns = o,\n",
        "                                num_num_assoc = weighted_pearson,\n",
        "                                nom_num_assoc = weighted_spearman,\n",
        "                                nom_nom_assoc = weighted_spearman,\n",
        "                                compute_only = True)[\"corr\"])\n",
        "  if len(c) + len(n) >= 2:\n",
        "    correlations.append(nom.associations(df[c + n], nominal_columns = n,\n",
        "                                num_num_assoc = weighted_pearson,\n",
        "                                nom_num_assoc = weighted_correlation_ratio,\n",
        "                                nom_nom_assoc = weighted_cramers_v,\n",
        "                                compute_only = True)[\"corr\"])\n",
        "  if len(n) + len(o) >= 2:\n",
        "    correlations.append(nom.associations(df[n + o], nominal_columns = n,\n",
        "                                num_num_assoc = weighted_spearman,\n",
        "                                nom_num_assoc = weighted_cramers_v,\n",
        "                                nom_nom_assoc = weighted_cramers_v,\n",
        "                                compute_only = True)[\"corr\"])\n",
        "\n",
        "  if not correlations:\n",
        "    return df, vars, ordinal, nominal, continuous\n",
        "\n",
        "  #combine the dfs in correlations\n",
        "  all_labels = list(set(n + c + o))\n",
        "  corr = pd.DataFrame(index=all_labels, columns=all_labels)\n",
        "\n",
        "  for d in correlations:\n",
        "    corr.update(d)\n",
        "\n",
        "  corr = corr.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "  #create a heat map\n",
        "  rounded_cols = math.ceil(len(df.columns) / 21) * 21\n",
        "  plt.figure(figsize=(rounded_cols*0.7,rounded_cols*0.6))\n",
        "  sns.heatmap(corr, annot=True)\n",
        "  plt.show()\n",
        "\n",
        "  if dropping:\n",
        "    #analysis on race/ethnicty + age in analysis section\n",
        "    '''age & educational attainment (schl) have perfect correlation, which makes\n",
        "    sense as the older someone is, the more education they can attain. both\n",
        "    features are kept as they are both strong indicators of income (older folks\n",
        "    have more work experience and thus, many times (but not always), can earn more,\n",
        "    and those with more advanced degress can also make more)'''\n",
        "    vars_to_keep = [\"RACE (OTHER)\", \"RACE\", \"ETHNICITY (OTHER)\", \"ETHNICITY\",\n",
        "                    \"ETHNICITY (CENSUS)\", \"AGEP\", \"SCHL\"]\n",
        "    vars_to_keep = [col for col in vars_to_keep if col in corr.columns]\n",
        "\n",
        "    vars_to_keep_drop = corr[vars_to_keep].apply(lambda col: (col >= threshold).any(), axis=1)\n",
        "    target_drop = corr[target] >= 0.95\n",
        "    filtered_corr = corr[vars_to_keep_drop | target_drop]\n",
        "    to_drop = filtered_corr.index.tolist()\n",
        "    to_drop = [col for col in to_drop if col not in vars_to_keep + [target]]\n",
        "    corr = corr.drop(index=to_drop, columns=to_drop)\n",
        "\n",
        "    #additional vars to drop based on correlation analysis\n",
        "    to_drop.extend([\"MAR\", \"RC\", \"RELSHIPP\"])\n",
        "\n",
        "    #update df, vars, ordinal, nominal, continuous\n",
        "    df.drop(columns=[col for col in to_drop if col in df], inplace=True)\n",
        "\n",
        "    vars = [x for x in vars if x not in to_drop]\n",
        "    ordinal = [x for x in ordinal if x not in to_drop]\n",
        "    nominal = [x for x in nominal if x not in to_drop]\n",
        "    continuous = [x for x in continuous if x not in to_drop]\n",
        "\n",
        "    #any remaining pairs above threshold?\n",
        "    remaining = False\n",
        "    cols = corr.columns\n",
        "    for i in range(len(cols)):\n",
        "      for j in range(i + 1, len(cols)):\n",
        "        col = cols[i]\n",
        "        idx = cols[j]\n",
        "        if col != idx and not (col in vars_to_keep + [target] or idx in vars_to_keep + [target]):\n",
        "          value = corr.loc[idx, col]\n",
        "          if abs(value) >= threshold:\n",
        "            print(f'{idx}, {col}: {value}')\n",
        "            remaining = True\n",
        "\n",
        "    if not remaining:\n",
        "      print(\"no remaining vars to eliminate\")\n",
        "\n",
        "  return df, vars, ordinal, nominal, continuous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLIGhJBBTR4M"
      },
      "outputs": [],
      "source": [
        "def weighted_mean(x, w):\n",
        "  '''weighted mean'''\n",
        "  return np.sum(x.astype(int) * w) / np.sum(w)\n",
        "\n",
        "def weighted_cov(x, y, w):\n",
        "  '''weighted covariance'''\n",
        "  return np.sum(w * (x.astype(int) - int(weighted_mean(x, w))) * (y - weighted_mean(y, w))) / np.sum(w)\n",
        "\n",
        "def weighted_pearson(x, y):\n",
        "  '''weighted pearson correlation; for continuous-continuous'''\n",
        "  #standardize\n",
        "  x = (x - x.mean()) / x.std()\n",
        "  y = (y - y.mean()) / y.std()\n",
        "\n",
        "  cov_xy = weighted_cov(x, y, weight_col)\n",
        "  cov_xx = weighted_cov(x, x, weight_col)\n",
        "  cov_yy = weighted_cov(y, y, weight_col)\n",
        "\n",
        "  return cov_xy / np.sqrt(cov_xx * cov_yy)\n",
        "\n",
        "def weighted_spearman(x, y):\n",
        "  '''weighted spearman's rank correlation; for continuous-ordinal + ordinal-ordinal'''\n",
        "  rx = rankdata(x)\n",
        "  ry = rankdata(y)\n",
        "\n",
        "  d = (rx - ry)\n",
        "  w_sum_d_squared = np.sum(weight_col * d**2)\n",
        "  w_sum_d_rank_squared = np.sum(weight_col * rx)**2\n",
        "\n",
        "  n = len(x)\n",
        "  return 1 - (6 * w_sum_d_squared) / (n * (n**2 - 1) * w_sum_d_rank_squared)\n",
        "\n",
        "def weighted_cramers_v(x, y):\n",
        "  '''weighted cramer's v; for categorical data; for ordinal-nominal & nominal-nominal'''\n",
        "\n",
        "  contingency_table = pd.crosstab(x, y, values=weight_col, aggfunc=np.sum, normalize=False).fillna(0)\n",
        "\n",
        "  chi2, _, _, _ = chi2_contingency(contingency_table)\n",
        "\n",
        "  total_weight = weight_col.sum()\n",
        "\n",
        "  R, C = contingency_table.shape\n",
        "  degrees_of_freedom = (R - 1) * (C - 1)\n",
        "\n",
        "  return np.sqrt(chi2 / (total_weight * min(R-1, C-1)))\n",
        "\n",
        "def weighted_correlation_ratio(x, y):\n",
        "  '''weighted correlation ratio; for nominal(x)-continuous(y)'''\n",
        "\n",
        "  df = pd.DataFrame({'x': x, 'y': y, 'weight': weight_col})\n",
        "\n",
        "  y_weighted_mean = weighted_mean(y, weight_col)\n",
        "  total_variance = (weight_col * (y - y_weighted_mean) ** 2).sum()\n",
        "\n",
        "  df['weighted_mean'] = df.groupby('x')['y'].transform(lambda x: np.sum(x * df.loc[x.index, 'weight']) / np.sum(df.loc[x.index, 'weight']))\n",
        "  df['squared_weighted_diff'] = (df['y'] - df['weighted_mean'])**2 * df['weight']\n",
        "  between_group_variance = df['squared_weighted_diff'].sum()\n",
        "\n",
        "  return between_group_variance / total_variance\n",
        "\n",
        "def target_corr(df, target, weight, measure):\n",
        "  '''\n",
        "  returns new df with index as col names and value as relationship with target\n",
        "  using the given weighted measure (a callable function)\n",
        "  '''\n",
        "  cols = df.columns\n",
        "  cols = cols.drop([target, weight])\n",
        "\n",
        "  l = len(cols)-1\n",
        "\n",
        "  results = []\n",
        "  for col in cols:\n",
        "    results.append(measure(df[col], df[target]))\n",
        "\n",
        "  results = pd.DataFrame(results, index = cols, columns=[\"corr\"])\n",
        "\n",
        "  results = results[\"corr\"].sort_values(ascending=False)[1:]\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53xGkXbJvWq9"
      },
      "source": [
        "### indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYhIYc-MG1po"
      },
      "outputs": [],
      "source": [
        "def index(df, vars):\n",
        "  '''\n",
        "  indexes specific vars and replaces the original vars with the indexes.\n",
        "  prints error message if not all rows properly recoded.\n",
        "\n",
        "  precondition: as is true in 2021, census microdata doesn't include puerto rico\n",
        "  '''\n",
        "  cols = df.columns\n",
        "\n",
        "  cols_to_drop = []\n",
        "\n",
        "  if \"ESP\" in cols:\n",
        "    df = index_parent_emp(df)\n",
        "    cols_to_drop.extend([\"ESP\"])\n",
        "\n",
        "  if \"HINS1\" in cols:\n",
        "    df = index_hlth_insur(df)\n",
        "    cols_to_drop.extend([\"HINS1\", \"HINS2\", \"HINS3\", \"HINS4\", \"HINS5\", \"HINS6\", \"HINS7\",\n",
        "                          \"HICOV\", \"PRIVCOV\", \"PUBCOV\", \"FHINS4C\", \"FHINS3C\", \"FHINS5C\"])\n",
        "\n",
        "  if \"OTHSVCEX\" in cols:\n",
        "    df = index_internet_acc(df)\n",
        "    cols_to_drop.extend([\"OTHSVCEX\", \"SATELLITE\", \"HISPEED\", \"ACCESSINET\", \"DIALUP\"])\n",
        "\n",
        "  if \"BROADBND\" in cols:\n",
        "    df = index_device_acc(df)\n",
        "    cols_to_drop.extend([\"BROADBND\", \"SMARTPHONE\", \"TEL\", \"TABLET\", \"LAPTOP\", \"COMPOTHX\"])\n",
        "\n",
        "  df = df.drop(cols_to_drop, axis=1)\n",
        "  vars = [x for x in vars if x not in cols_to_drop]\n",
        "\n",
        "  return df, vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMisoiUlbUHp"
      },
      "outputs": [],
      "source": [
        "def index_parent_emp(df):\n",
        "  '''\n",
        "  esp: employment status of parents\n",
        "  0 (0) -- not own child of householder, and not child in subfamily\n",
        "  1 (1) -- 2 parents, both in labor force\n",
        "  2 (2,3) - 2 parents, one in labor force\n",
        "  3 (5, 7) -- single parent, in labor force\n",
        "  4 (6, 8) -- single parent, not in labor force\n",
        "  5 (4) - 2 parents, neither in labor force\n",
        "  '''\n",
        "  col = \"PARENT_EMP\"\n",
        "  df.loc[df['ESP'] == 0, col] = 0\n",
        "  df.loc[df['ESP'] == 1, col] = 1\n",
        "  df.loc[df['ESP'].isin([2, 3]), col] = 2\n",
        "  df.loc[df['ESP'].isin([5, 7]), col] = 3\n",
        "  df.loc[df['ESP'].isin([6, 8]), col] = 4\n",
        "  df.loc[df['ESP'] == 4, col] = 5\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def index_hlth_insur(df):\n",
        "  '''\n",
        "  0 - no health insurance (HICOV = 2, HINS7 = 2)\n",
        "  1 - indian health service (HINS7 = 1)\n",
        "  2 - public health insurance (PUBCOV = 1)\n",
        "  3 - private health insurance (PRIVCOV = 1)\n",
        "  census health insurance recode documentation: https://www.census.gov/topics/health/health-insurance/guidance/programming-code/acs-recoding.html\n",
        "  '''\n",
        "  col = \"HLTH_INSUR\"\n",
        "  df.loc[(df['HICOV'] == 2) & (df['HINS7'] == 2), col] = 0\n",
        "  df.loc[df['HINS7'] == 1, col] = 1\n",
        "  df.loc[df['PUBCOV'] == 1, col] = 2\n",
        "  df.loc[df['PRIVCOV'] == 1, col] = 3\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def index_internet_acc(df):\n",
        "  '''\n",
        "  0 - n/a (group quarters (e.g. dorm) or vacant)\n",
        "  1 - no internet (ACCESSINET = 3)\n",
        "  2 - unpaid internet (ACCESSINET = 2)\n",
        "  3 - dial up (DIALUP = 1)\n",
        "  4 - internet thru cellphone company (ACCESSINET = 1 & others = 2)\n",
        "  5 - satellite/other (SATELLITE = 1 or OTHSVCEX = 1)\n",
        "  6 - hispeed (HISPEED = 1)\n",
        "  '''\n",
        "  col = \"INTERNET_ACC\"\n",
        "  df.loc[(df['OTHSVCEX'] == 0) & (df['SATELLITE'] == 0) &\n",
        "              (df['HISPEED'] == 0) & (df['ACCESSINET'] == 0), col] = 0\n",
        "  df.loc[df['ACCESSINET'] == 3, col] = 1\n",
        "  df.loc[df['ACCESSINET'] == 2, col] = 2\n",
        "  df.loc[(df['DIALUP'] == 1), col] = 3\n",
        "  df.loc[(df['OTHSVCEX'] == 2) & (df['SATELLITE'] == 2) &\n",
        "              ((df['HISPEED'] == 2) & (df['ACCESSINET'] == 1)), col] = 4\n",
        "  df.loc[(df['SATELLITE'] == 1) | (df['OTHSVCEX'] == 1), col] = 5\n",
        "  df.loc[df['HISPEED'] == 1, col] = 6\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def index_device_acc(df):\n",
        "  '''\n",
        "  0 - n/a (group quarters (e.g. dorm) or vacant)\n",
        "  1 - no devices (TEL = 2, SMARTPHONE = 2, TABLET = 2, LAPTOP = 2)\n",
        "  2 - telephone access\n",
        "  3 - laptop or tablet\n",
        "  4 - smartphone access\n",
        "  5 - smartphone access + laptop or tablet\n",
        "  6 - smartphone with data (BROADBND = 1, SMARTPHONE = 1)\n",
        "  7 - smartphone with data + laptop or tablet\n",
        "\n",
        "  note: tried recoding taking the value of COMPOTHX into account, and it didn't\n",
        "  make a difference\n",
        "  '''\n",
        "  col = \"DEVICE_ACC\"\n",
        "\n",
        "  computer = ((df['TABLET'] == 1) | (df['LAPTOP'] == 1))\n",
        "\n",
        "  df.loc[(df['SMARTPHONE'] == 1) & (df['BROADBND'] == 1) &\n",
        "              computer, col] = 7\n",
        "  df.loc[(df['SMARTPHONE'] == 1) & (df['BROADBND'] == 1), col] = 6\n",
        "  df.loc[(df['SMARTPHONE'] == 1) & computer, col] = 5\n",
        "  df.loc[(df['SMARTPHONE'] == 1), col] = 4\n",
        "  df.loc[computer, col] = 3\n",
        "  df.loc[df['TEL'] == 1, col] = 2\n",
        "  df.loc[(df['TEL'] == 2) & (df['SMARTPHONE'] == 2) &\n",
        "          (df['TABLET'] == 2) & (df['LAPTOP'] == 2), col] = 1\n",
        "\n",
        "  df.loc[(df['SMARTPHONE'] == 0) & (df['BROADBND'] == 0) &\n",
        "              ~computer, col] = 0\n",
        "\n",
        "  if df[col].isna().any():\n",
        "    print(\"error: \" + col + \" column not filled\")\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g174jMY8HvSB"
      },
      "source": [
        "### ethnoracial vars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R58aNrDtq4IV"
      },
      "outputs": [],
      "source": [
        "def create_ethnoracial_vars(df, nominal):\n",
        "  '''\n",
        "  creates new variables and renames existing ethnoracial vars to enable analysis\n",
        "  of different approaches to coding data on multiracial and multiethnic people\n",
        "\n",
        "  RACE (OTHER): the census's RAC1P variable — race classification with multiracial\n",
        "  people coded as Two or More Races\n",
        "  1 -- Asian alone\n",
        "  2 -- Native Hawaiian and Other Pacific Islander alone\n",
        "  3 -- Two or More Races\n",
        "\n",
        "  RACE - RACE (OTHER) but with the most specific information on race combinations\n",
        "  possible for multiracial people\n",
        "  1 -- Asian alone\n",
        "  2 -- Native Hawaiian and Other Pacific Islander alone\n",
        "  3 -- Asian, Native Hawaiian and Other Pacific Islander\n",
        "  4 -- Asian, White\n",
        "  5 -- Native Hawaiian and Other Pacific Islander, White\n",
        "  6 -- Asian, Black or African American\n",
        "  7 -- Native Hawaiian and Other Pacific Islander, Black or African American\n",
        "  8 -- Asian, American Indian and Alaska Native\n",
        "  9 -- Asian, Some Other Race\n",
        "  10 -- Native Hawaiian and Other Pacific Islander, and/or Some Other Race\n",
        "  11 -- Asian, White, Black or African American\n",
        "  12 -- Asian, White, American Indian and Alaska Native\n",
        "  13 -- Asian, White, Native Hawaiian and Other Pacific Islander\n",
        "  14 -- Asian, and/or White, and/or Native Hawaiian and Other Pacific Islander,\n",
        "  and/or Some Other Race\n",
        "  15 -- Native Hawaiian and Other Pacific Islander, White, and/or Some Other Race\n",
        "  16 -- Black or African American, American Indian and Alaska Native, and/or Asian,\n",
        "  and/or Native Hawaiian and Other Pacific Islander, and/or Some Other Race\n",
        "  17 -- Asian, Black or African American, and/or Native Hawaiian and Other Pacific\n",
        "  Islander, and/or Some Other Race\n",
        "  18 -- Asian, American Indian and Alaska Native, and/or Native Hawaiian and Other\n",
        "  Pacific Islander, and/or Some Other Race\n",
        "  19 -- Asian, and/or Native Hawaiian and Other Pacific Islander groups, and/or\n",
        "  Some Other Race\n",
        "  20 -- White, and/or Black or African American, and/or American Indian and Alaska\n",
        "  Native, and/or Asian, and/or Native Hawaiian and Other Pacific Islander, and/or\n",
        "  Some Other Race\n",
        "\n",
        "  ETHNICITY (OTHER): the census's RAC2P variable - ethnicity classification with\n",
        "  multiracial/multiethnic coded as either All combinations of Asian races only,\n",
        "  Other Native Hawaiian and Other Pacific Islander, or Two or More Races. ethnicites\n",
        "  with smaller population sizes coded as Other Asian alone or Other Native Hawaiian\n",
        "  and Other Pacific Islander.\n",
        "  1 -- Asian Indian alone\n",
        "  2 -- Bangladeshi alone\n",
        "  ...\n",
        "  30 -- Two or More Races\n",
        "  (38-68 of RAC2P (excluding 67), recoded consecutively. see data dictionary for\n",
        "  rest of values)\n",
        "\n",
        "  ETHNICITY (CENSUS): the census's RAC3P variable - ethnicity classification with\n",
        "  multiracial/multiethnic coded as their specific ethnicity combination in many\n",
        "  cases but missing certain individual ethnicities and ethnicity combinations\n",
        "  for those with small population sizes\n",
        "  1 -- Asian Indian alone\n",
        "  2 -- Chinese alone\n",
        "  ...\n",
        "  87 -- White; and/or Black or African American; and/or American Indian and\n",
        "  Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific\n",
        "  Islander groups; and/or Some Other Race\n",
        "  (4-100 of RAC3P (excluding 15, 16, 17, 29, 30, 38, 41, 60, 62, 64), recoded\n",
        "  consecutively. see data dictionary for rest of values)\n",
        "\n",
        "  ETHNICITY: combines all information provided by ETHNICITY (OTHER) and ETHNICITY (CENSUS)\n",
        "  to provide the most specific information on ethnicity combinations possible.\n",
        "  1 -- Asian Indian alone\n",
        "  2 -- Bangladeshi alone\n",
        "  ...\n",
        "  105 -- White; and/or Black or African American; and/or American Indian and\n",
        "  Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific\n",
        "  Islander groups; and/or Some Other Race\n",
        "  (see data dictionary for rest of values)\n",
        "\n",
        "  note: the language of categories are in line with the census's.\n",
        "  '''\n",
        "\n",
        "  #renaming census vars\n",
        "  df = df.rename(columns={'RAC1P': 'RACE (OTHER)', 'RAC2P': 'ETHNICITY (OTHER)',\n",
        "                          'RAC3P': 'ETHNICITY (CENSUS)'})\n",
        "\n",
        "  #recoding RACE (OTHER)\n",
        "  df['RACE (OTHER)'] = df['RACE (OTHER)'].replace({6: 1, 7: 2, 9: 3})\n",
        "\n",
        "  #creating RACE\n",
        "  ethnicities_to_race = {4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:2, 12:2, 13:2,\n",
        "                          14:2, 18:4, 19:4, 20:4, 21:4, 22:4, 23:4, 24:4, 25:5,\n",
        "                          26:5, 27:5, 28:5, 31:6, 32:6, 33:6, 34:6, 35:6, 36:6,\n",
        "                          37:7, 39:8, 40:8, 42:1, 43:9, 44:1, 45:1, 46:1, 47:1,\n",
        "                          48:1, 49:3, 50:1, 51:3, 52:3, 53:9, 54:1, 55:3, 56:1,\n",
        "                          57:3, 58:9, 59:10, 61:11, 63:12, 65:4, 66:4, 67:13,\n",
        "                          68:13, 69:13, 70:14, 71:3, 72:13, 73:13, 74:6, 75:7,\n",
        "                          76:1, 77:1, 78:11, 79:12, 80:15, 81:20, 82:20, 83:20,\n",
        "                          84:20, 85:14, 86:14, 87:14, 88:14, 89:14, 90:16, 91:17,\n",
        "                          92:18, 93:14, 94:3, 95:19, 96:19, 97:19, 98:19, 99:10,\n",
        "                          100:20}\n",
        "  df['RACE'] = df.apply(lambda row: ethnicities_to_race.get(row['ETHNICITY (CENSUS)'],\n",
        "                                                            row['RACE (OTHER)']) if\n",
        "                        row['RACE (OTHER)'] == 3 else row['RACE (OTHER)'], axis=1)\n",
        "\n",
        "  #recoding ETHNICITY (OTHER)\n",
        "  df['ETHNICITY (OTHER)'] = pd.factorize(df['ETHNICITY (OTHER)'],sort=True)[0] + 1\n",
        "\n",
        "  #recoding ETHNICITY (CENSUS)\n",
        "  df['ETHNICITY (CENSUS)'] = pd.factorize(df['ETHNICITY (CENSUS)'],sort=True)[0] + 1\n",
        "\n",
        "  #creating ETHNICITY\n",
        "  df['ETHNICITY'] = df.apply(lambda row: row['ETHNICITY (OTHER)'] if row['ETHNICITY (OTHER)']\n",
        "                             not in [22,29,30] else row['ETHNICITY (CENSUS)']+30, axis=1)\n",
        "  df['ETHNICITY'] = pd.factorize(df['ETHNICITY'],sort=True)[0] + 1\n",
        "\n",
        "  #update nominal\n",
        "  nominal.extend([\"RACE (OTHER)\", \"RACE\", \"ETHNICITY (OTHER)\", \"ETHNICITY (CENSUS)\", \"ETHNICITY\"])\n",
        "  nominal = [x for x in nominal if x not in ['RAC1P','RAC2P', 'RAC3P']]\n",
        "\n",
        "  return df, nominal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7w0JOxNY2lR"
      },
      "source": [
        "### lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46U6PRMEJvfy"
      },
      "outputs": [],
      "source": [
        "def lasso(data, target):\n",
        "  x = data.drop([target, housing_identifier, person_identifier], axis=1)\n",
        "  x_weight = x[weight]\n",
        "  x = x.drop(weight, axis=1)\n",
        "  y = data[target]\n",
        "\n",
        "  #split into test & train\n",
        "  x_train, x_test, y_train, y_test, train_weights, test_weights = train_test_split(x, y, x_weight, test_size=0.2, random_state=42)\n",
        "\n",
        "  #standardize data\n",
        "  scaler = StandardScaler()\n",
        "  x_train = scaler.fit_transform(x_train)\n",
        "  x_test = scaler.transform(x_test)\n",
        "\n",
        "  #gridsearchcv to get best alpha\n",
        "  param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]}\n",
        "  lasso = Lasso()\n",
        "  grid_search = GridSearchCV(lasso, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "  grid_search.fit(x_train, y_train, sample_weight=train_weights)\n",
        "  best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "  #train model on best alpha\n",
        "  final_model = Lasso(alpha=best_alpha)\n",
        "  final_model.fit(x_train, y_train, sample_weight=train_weights)\n",
        "  y_pred = final_model.predict(x_test)\n",
        "  final_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "  #print\n",
        "  print(\"best alpha:\", best_alpha)\n",
        "  print(\"final mse:\", final_mse)\n",
        "\n",
        "  #return features\n",
        "  coefficients = pd.Series(final_model.coef_, index=x.columns)\n",
        "  return coefficients[coefficients != 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLD7vZ8uEAM"
      },
      "source": [
        "## execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unTo2cmq29aB"
      },
      "outputs": [],
      "source": [
        "limiters = [\"&RACPI=1&RACASN=1&RACNH=1\", \"&RACPI=1&RACASN=1&RACNH=0\",\n",
        "            \"&RACPI=1&RACASN=0&RACNH=1\", \"&RACPI=1&RACASN=0&RACNH=0\",\n",
        "            \"&RACPI=0&RACASN=1&RACNH=1\", \"&RACPI=0&RACASN=1&RACNH=0\",\n",
        "            \"&RACPI=0&RACASN=0&RACNH=1\"] #all aapi\n",
        "drop = [\"RACPI\", \"RACASN\", \"RACNH\"] #vars used for limiters\n",
        "data, vars, ordinal, nominal, continuous = import_census_data(vars, \"2021\", limiters, False, drop, ordinal, nominal, continuous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC95XA56mxZD"
      },
      "outputs": [],
      "source": [
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QCyRq_sadpT"
      },
      "outputs": [],
      "source": [
        "drop = [\"ETHNICITY\", \"RACE (OTHER)\", \"ETHNICITY (CENSUS)\"] #have high correlation with the other ethnoracial vars, which affects lasso\n",
        "features = lasso(data.drop(drop, axis=1), target)\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceWqV2P4MEKJ"
      },
      "outputs": [],
      "source": [
        "data = data[features.index.tolist() + [target, housing_identifier, person_identifier, weight] + drop] #select features based on lasso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combine ids\n",
        "if 'ID' not in data.columns:\n",
        "  data['ID'] = data[housing_identifier].astype(str) + data[person_identifier].astype(str)\n",
        "  data.drop([housing_identifier, person_identifier],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "E43cBjyNa2G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rename variables\n",
        "rename_dict = {'GCL':'GRANDPARENTS IN HOUSEHOLD (30+)',\n",
        "               'WRK':'WORKED LAST WK',\n",
        "               'POBP':'PLACE OF BIRTH',\n",
        "               'FOD1P':'FIELD OF DEGREE',\n",
        "               'INDP':'INDUSTRY',\n",
        "               'JWTRNS':'TRANSPORTATION TO WORK',\n",
        "               'POWPUMA':'PLACE OF WORK (PUMA AREA)',\n",
        "               'SCHL': 'EDUCATIONAL ATTAINMENT',\n",
        "               'ESR':'EMPLOYMENT STATUS',\n",
        "               'DOUT':'INDEPENDENT LIVING DIFFICULTY',\n",
        "               'POWSP':'PLACE OF WORK (STATE/COUNTRY)',\n",
        "               'MIL':'MILITARY SERVICE',\n",
        "               'OC':'HAS CHILD',\n",
        "               'MARHW':'WIDOWED (IN PAST YR)',\n",
        "               'OCCP':'OCCUPATION',\n",
        "               'MARHD':'DIVORCED (IN PAST YR)',\n",
        "               'MSP':'MARITAL STATUS',\n",
        "               'MARHM':'MARRIED (IN PAST YR)',\n",
        "               'COW':'EMPLOYEE TYPE',\n",
        "               'NOP':'NATIVITY OF PARENTS (U18)',\n",
        "               'AGEP':'AGE',\n",
        "               'PWGTP':'SAMPLE WEIGHT',\n",
        "               'PINCP':'INCOME'}\n",
        "\n",
        "cols = data.columns\n",
        "vars = [rename_dict.get(v, v) for v in vars if v in cols]\n",
        "nominal = [rename_dict.get(n, n) for n in nominal if n in cols]\n",
        "continuous = [rename_dict.get(c, c) for c in continuous if c in cols]\n",
        "ordinal = [rename_dict.get(o, o) for o in ordinal if o in cols]\n",
        "\n",
        "data.rename(columns=rename_dict, inplace=True)"
      ],
      "metadata": {
        "id": "t5gc-rIiXacR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "AyOM1HrmeQ1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9anW61PpMWnp"
      },
      "outputs": [],
      "source": [
        "#heat map for selected features (excluding id, weight, and income)\n",
        "global target\n",
        "target = 'INCOME'\n",
        "if target in data:\n",
        "  d = data.drop(target,axis=1)\n",
        "else:\n",
        "  d = data.copy()\n",
        "d, v, o, n, c = create_corr_heat_map(d, vars, ordinal, nominal, continuous, 0.9, False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "id": "YlZMpmY-l8dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['ID', 'SAMPLE WEIGHT',\n",
        "             'AGE', 'PLACE OF BIRTH', 'NATIVITY OF PARENTS (U18)',\n",
        "             'MARITAL STATUS', 'MARRIED (IN PAST YR)', 'DIVORCED (IN PAST YR)', 'WIDOWED (IN PAST YR)',\n",
        "             'HAS CHILD', 'GRANDPARENTS IN HOUSEHOLD (30+)', 'INDEPENDENT LIVING DIFFICULTY',\n",
        "             'RACE (OTHER)',\n",
        "             'RACE', 'ETHNICITY (OTHER)', 'ETHNICITY (CENSUS)',\n",
        "             'ETHNICITY',\n",
        "             'INCOME',  'OCCUPATION', 'INDUSTRY', 'EMPLOYMENT STATUS', 'EMPLOYEE TYPE', 'WORKED LAST WK',\n",
        "             'PLACE OF WORK (PUMA AREA)', 'PLACE OF WORK (STATE/COUNTRY)',\n",
        "             'MILITARY SERVICE', 'TRANSPORTATION TO WORK', 'EDUCATIONAL ATTAINMENT', 'FIELD OF DEGREE']]"
      ],
      "metadata": {
        "id": "ud89ExD7lw5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "S9uqdXmL1Jdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save final dataset\n",
        "data.to_csv('/content/drive/My Drive/data.csv', index=False)"
      ],
      "metadata": {
        "id": "s4pn7DLvdNfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IHYuuVUjuJp"
      },
      "source": [
        "# analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get dataset\n",
        "data = pd.read_csv('/content/drive/My Drive/data.csv')"
      ],
      "metadata": {
        "id": "PfLYXTKB1Top"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#map ethnoracial numerical values to categories\n",
        "race_other_mapping = {1: 'Asian alone',\n",
        "    2: 'Native Hawaiian and Other Pacific Islander alone',\n",
        "    3: 'Two or More Races'}\n",
        "\n",
        "race_mapping = {1: 'Asian alone',\n",
        "    2: 'Native Hawaiian and Other Pacific Islander alone',\n",
        "    3: 'Asian, Native Hawaiian and Other Pacific Islander',\n",
        "    4: 'Asian, White',\n",
        "    5: 'Native Hawaiian and Other Pacific Islander, White',\n",
        "    6: 'Asian, Black or African American',\n",
        "    7: 'Native Hawaiian and Other Pacific Islander, Black or African American',\n",
        "    8: 'Asian, American Indian and Alaska Native',\n",
        "    9: 'Asian, Some Other Race',\n",
        "    10: 'Native Hawaiian and Other Pacific Islander, and/or Some Other Race',\n",
        "    11: 'Asian, White, Black or African American',\n",
        "    12: 'Asian, White, American Indian and Alaska Native',\n",
        "    13: 'Asian, White, Native Hawaiian and Other Pacific Islander',\n",
        "    14: 'Asian, and/or White, and/or Native Hawaiian and Other Pacific Islander, and/or Some Other Race',\n",
        "    15: 'Native Hawaiian and Other Pacific Islander, White, and/or Some Other Race',\n",
        "    16: 'Black or African American, American Indian and Alaska Native, and/or Asian, and/or Native Hawaiian and Other Pacific Islander, and/or Some Other Race',\n",
        "    17: 'Asian, Black or African American, and/or Native Hawaiian and Other Pacific Islander, and/or Some Other Race',\n",
        "    18: 'Asian, American Indian and Alaska Native, and/or Native Hawaiian and Other Pacific Islander, and/or Some Other Race',\n",
        "    19: 'Asian, and/or Native Hawaiian and Other Pacific Islander groups, and/or Some Other Race',\n",
        "    20: 'White, and/or Black or African American, and/or American Indian and Alaska Native, and/or Asian, and/or Native Hawaiian and Other Pacific Islander, and/or Some Other Race'}\n",
        "\n",
        "ethnicity_other_mapping = {1: 'Asian Indian alone',\n",
        "    2: 'Bangladeshi alone',\n",
        "    3: 'Bhutanese alone',\n",
        "    4: 'Burmese alone',\n",
        "    5: 'Cambodian alone',\n",
        "    6: 'Chinese, except Taiwanese, alone',\n",
        "    7: 'Taiwanese alone',\n",
        "    8: 'Filipino alone',\n",
        "    9: 'Hmong alone',\n",
        "    10: 'Indonesian alone',\n",
        "    11: 'Japanese alone',\n",
        "    12: 'Korean alone',\n",
        "    13: 'Laotian alone',\n",
        "    14: 'Malaysian alone',\n",
        "    15: 'Mongolian alone',\n",
        "    16: 'Nepalese alone',\n",
        "    17: 'Pakistani alone',\n",
        "    18: 'Sri Lankan alone',\n",
        "    19: 'Thai alone',\n",
        "    20: 'Vietnamese alone',\n",
        "    21: 'Other Asian alone',\n",
        "    22: 'All combinations of Asian races only',\n",
        "    23: 'Native Hawaiian alone',\n",
        "    24: 'Samoan alone',\n",
        "    25: 'Tongan alone',\n",
        "    26: 'Chamorro alone',\n",
        "    27: 'Marshallese alone',\n",
        "    28: 'Fijian alone',\n",
        "    29: 'Other Native Hawaiian and Other Pacific Islander',\n",
        "    30: 'Two or More Races'}\n",
        "\n",
        "ethnicity_census_mapping = {1: 'Asian Indian alone',\n",
        "    2: 'Chinese alone',\n",
        "    3: 'Filipino alone',\n",
        "    4: 'Japanese alone',\n",
        "    5: 'Korean alone',\n",
        "    6: 'Vietnamese alone',\n",
        "    7: 'Other Asian alone',\n",
        "    8: 'Native Hawaiian alone',\n",
        "    9: 'Chamorro alone',\n",
        "    10: 'Samoan alone',\n",
        "    11: 'Other Pacific Islander alone',\n",
        "    12: 'White; Asian Indian',\n",
        "    13: 'White; Chinese',\n",
        "    14: 'White; Filipino',\n",
        "    15: 'White; Japanese',\n",
        "    16: 'White; Korean',\n",
        "    17: 'White; Vietnamese',\n",
        "    18: 'White; Other Asian',\n",
        "    19: 'White; Native Hawaiian',\n",
        "    20: 'White; Chamorro',\n",
        "    21: 'White; Samoan',\n",
        "    22: 'White; Other Pacific Islander',\n",
        "    23: 'Black or African American; Asian Indian',\n",
        "    24: 'Black or African American; Chinese',\n",
        "    25: 'Black or African American; Filipino',\n",
        "    26: 'Black or African American; Japanese',\n",
        "    27: 'Black or African American; Korean',\n",
        "    28: 'Black or African American; Other Asian',\n",
        "    29: 'Black or African American; Other Pacific Islander',\n",
        "    30: 'American Indian and Alaska Native; Asian Indian',\n",
        "    31: 'American Indian and Alaska Native; Filipino',\n",
        "    32: 'Asian Indian; Other Asian',\n",
        "    33: 'Asian Indian; Some Other Race',\n",
        "    34: 'Chinese; Filipino',\n",
        "    35: 'Chinese; Japanese',\n",
        "    36: 'Chinese; Korean',\n",
        "    37: 'Chinese; Vietnamese',\n",
        "    38: 'Chinese; Other Asian',\n",
        "    39: 'Chinese; Native Hawaiian',\n",
        "    40: 'Filipino; Japanese',\n",
        "    41: 'Filipino; Native Hawaiian',\n",
        "    42: 'Filipino; Other Pacific Islander',\n",
        "    43: 'Filipino; Some Other Race',\n",
        "    44: 'Japanese; Korean',\n",
        "    45: 'Japanese; Native Hawaiian',\n",
        "    46: 'Vietnamese; Other Asian',\n",
        "    47: 'Other Asian; Other Pacific Islander',\n",
        "    48: 'Other Asian; Some Other Race',\n",
        "    49: 'Other Pacific Islander; Some Other Race',\n",
        "    50: 'White; Black or African American; Filipino',\n",
        "    51: 'White; American Indian and Alaska Native; Filipino',\n",
        "    52: 'White; Chinese; Filipino',\n",
        "    53: 'White; Chinese; Japanese',\n",
        "    54: 'White; Chinese; Native Hawaiian',\n",
        "    55: 'White; Filipino; Native Hawaiian',\n",
        "    56: 'White; Japanese; Native Hawaiian',\n",
        "    57: 'White; Other Asian; Some Other Race',\n",
        "    58: 'Chinese; Filipino; Native Hawaiian',\n",
        "    59: 'White; Chinese; Filipino; Native Hawaiian',\n",
        "    60: 'White; Chinese; Japanese; Native Hawaiian',\n",
        "    61: 'Black or African American; Asian groups',\n",
        "    62: 'Black or African American; Native Hawaiian and Other Pacific Islander groups',\n",
        "    63: 'Asian Indian; Asian groups',\n",
        "    64: 'Filipino; Asian groups',\n",
        "    65: 'White; Black or African American; Asian groups',\n",
        "    66: 'White; American Indian and Alaska Native; Asian groups',\n",
        "    67: 'White; Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    68: 'White; Black or African American; American Indian and Alaska Native; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    69: 'White; Black or African American; American Indian and Alaska Native; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    70: 'White; Black or African American; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    71: 'White; American Indian and Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups',\n",
        "    72: 'White; Chinese; Filipino; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    73: 'White; Chinese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    74: 'White; Filipino; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    75: 'White; Japanese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    76: 'White; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    77: 'Black or African American; American Indian and Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    78: 'Black or African American; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    79: 'American Indian and Alaska Native; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    80: 'Asian Indian; and/or White; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    81: 'Chinese; Japanese; Native Hawaiian; and/or other Asian and/or Pacific Islander groups',\n",
        "    82: 'Chinese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    83: 'Filipino; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    84: 'Japanese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    85: 'Korean; and/or Vietnamese; and/or Other Asian; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    86: 'Native Hawaiian; and/or Pacific Islander groups; and/or Some Other Race',\n",
        "    87: 'White; and/or Black or African American; and/or American Indian and Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race'}\n",
        "\n",
        "ethnicity_mapping = {1: 'Asian Indian alone',\n",
        "    2: 'Bangladeshi alone',\n",
        "    3: 'Bhutanese alone',\n",
        "    4: 'Burmese alone',\n",
        "    5: 'Cambodian alone',\n",
        "    6: 'Chinese, except Taiwanese, alone',\n",
        "    7: 'Taiwanese alone',\n",
        "    8: 'Filipino alone',\n",
        "    9: 'Hmong alone',\n",
        "    10: 'Indonesian alone',\n",
        "    11: 'Japanese alone',\n",
        "    12: 'Korean alone',\n",
        "    13: 'Laotian alone',\n",
        "    14: 'Malaysian alone',\n",
        "    15: 'Mongolian alone',\n",
        "    16: 'Nepalese alone',\n",
        "    17: 'Pakistani alone',\n",
        "    18: 'Sri Lankan alone',\n",
        "    19: 'Thai alone',\n",
        "    20: 'Vietnamese alone',\n",
        "    21: 'Other Asian alone',\n",
        "    22: 'Native Hawaiian alone',\n",
        "    23: 'Samoan alone',\n",
        "    24: 'Tongan alone',\n",
        "    25: 'Chamorro alone',\n",
        "    26: 'Marshallese alone',\n",
        "    27: 'Fijian alone',\n",
        "    28: 'Other Native Hawaiian and Other Pacific Islander',\n",
        "    29: 'Other Pacific Islander alone',\n",
        "    30: 'White; Asian Indian',\n",
        "    31: 'White; Chinese',\n",
        "    32: 'White; Filipino',\n",
        "    33: 'White; Japanese',\n",
        "    34: 'White; Korean',\n",
        "    35: 'White; Vietnamese',\n",
        "    36: 'White; Other Asian',\n",
        "    37: 'White; Native Hawaiian',\n",
        "    38: 'White; Chamorro',\n",
        "    39: 'White; Samoan',\n",
        "    40: 'White; Other Pacific Islander',\n",
        "    41: 'Black or African American; Asian Indian',\n",
        "    42: 'Black or African American; Chinese',\n",
        "    43: 'Black or African American; Filipino',\n",
        "    44: 'Black or African American; Japanese',\n",
        "    45: 'Black or African American; Korean',\n",
        "    46: 'Black or African American; Other Asian',\n",
        "    47: 'Black or African American; Other Pacific Islander',\n",
        "    48: 'American Indian and Alaska Native; Asian Indian',\n",
        "    49: 'American Indian and Alaska Native; Filipino',\n",
        "    50: 'Asian Indian; Other Asian',\n",
        "    51: 'Asian Indian; Some Other Race',\n",
        "    52: 'Chinese; Filipino',\n",
        "    53: 'Chinese; Japanese',\n",
        "    54: 'Chinese; Korean',\n",
        "    55: 'Chinese; Vietnamese',\n",
        "    56: 'Chinese; Other Asian',\n",
        "    57: 'Chinese; Native Hawaiian',\n",
        "    58: 'Filipino; Japanese',\n",
        "    59: 'Filipino; Native Hawaiian',\n",
        "    60: 'Filipino; Other Pacific Islander',\n",
        "    61: 'Filipino; Some Other Race',\n",
        "    62: 'Japanese; Korean',\n",
        "    63: 'Japanese; Native Hawaiian',\n",
        "    64: 'Vietnamese; Other Asian',\n",
        "    65: 'Other Asian; Other Pacific Islander',\n",
        "    66: 'Other Asian; Some Other Race',\n",
        "    67: 'Other Pacific Islander; Some Other Race',\n",
        "    68: 'White; Black or African American; Filipino',\n",
        "    69: 'White; American Indian and Alaska Native; Filipino',\n",
        "    70: 'White; Chinese; Filipino',\n",
        "    71: 'White; Chinese; Japanese',\n",
        "    72: 'White; Chinese; Native Hawaiian',\n",
        "    73: 'White; Filipino; Native Hawaiian',\n",
        "    74: 'White; Japanese; Native Hawaiian',\n",
        "    75: 'White; Other Asian; Some Other Race',\n",
        "    76: 'Chinese; Filipino; Native Hawaiian',\n",
        "    77: 'White; Chinese; Filipino; Native Hawaiian',\n",
        "    78: 'White; Chinese; Japanese; Native Hawaiian',\n",
        "    79: 'Black or African American; Asian groups',\n",
        "    80: 'Black or African American; Native Hawaiian and Other Pacific Islander groups',\n",
        "    81: 'Asian Indian; Asian groups',\n",
        "    82: 'Filipino; Asian groups',\n",
        "    83: 'White; Black or African American; Asian groups',\n",
        "    84: 'White; American Indian and Alaska Native; Asian groups',\n",
        "    85: 'White; Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    86: 'White; Black or African American; American Indian and Alaska Native; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    87: 'White; Black or African American; American Indian and Alaska Native; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    88: 'White; Black or African American; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    89: 'White; American Indian and Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups',\n",
        "    90: 'White; Chinese; Filipino; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    91: 'White; Chinese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    92: 'White; Filipino; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    93: 'White; Japanese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    94: 'White; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    95: 'Black or African American; American Indian and Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    96: 'Black or African American; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    97: 'American Indian and Alaska Native; Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    98: 'Asian Indian; and/or White; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    99: 'Chinese; Japanese; Native Hawaiian; and/or other Asian and/or Pacific Islander groups',\n",
        "    100: 'Chinese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    101: 'Filipino; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    102: 'Japanese; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    103: 'Korean; and/or Vietnamese; and/or Other Asian; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race',\n",
        "    104: 'Native Hawaiian; and/or Pacific Islander groups; and/or Some Other Race',\n",
        "    105: 'White; and/or Black or African American; and/or American Indian and Alaska Native; and/or Asian groups; and/or Native Hawaiian and Other Pacific Islander groups; and/or Some Other Race'}"
      ],
      "metadata": {
        "id": "3af-Oxpnviwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ihZrIRj7MQ"
      },
      "source": [
        "## for race/ethnicity + age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSBEvzYlLfWH"
      },
      "outputs": [],
      "source": [
        "'''age and race should not have a high correlation when looking at the entire aapi\n",
        "population. however, differences in mean age across population (affected by different population sizes\n",
        "and the fact that multiracial/ethnic people tend to be younger\n",
        "(https://www.census.gov/library/stories/2023/06/nearly-a-third-reporting-two-or-more-races-under-18-in-2020.html))\n",
        "affects how correlation ratio is calculated, resulting in a much higher relationship measured\n",
        "between age and race/ethnicity. cramer's v doesn't have that same issue since it\n",
        "is just checking for strong associations between certain races/ethnicities and\n",
        "specific age groups, which are not present. thus, age is better treated as ordinal instead\n",
        "of continuous in the context of this research question''';\n",
        "\n",
        "age_means_by_ethnicity = data.groupby('ETHNICITY (OTHER)')['AGE'].mean().sort_values(ascending=False)\n",
        "\n",
        "age_means_by_ethnicity_df = age_means_by_ethnicity.reset_index()\n",
        "age_means_by_ethnicity_df['ETHNICITY (OTHER)'] = age_means_by_ethnicity_df['ETHNICITY (OTHER)'].map(ethnicity_other_mapping)\n",
        "print(age_means_by_ethnicity_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-UHHqzRjynC"
      },
      "source": [
        "## for indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LewCoCCI7f7T"
      },
      "outputs": [],
      "source": [
        "def request_data(request, limiters=[]):\n",
        "  dfs = []\n",
        "\n",
        "  if limiters == []: #if no limiters\n",
        "    response = requests.get(request)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    return df.rename(columns=df.iloc[0]).loc[1:].reset_index()\n",
        "\n",
        "  for l in limiters:\n",
        "    response = requests.get(request+l)\n",
        "    json_data = json.dumps(response.json())\n",
        "    df = pd.read_json(json_data)\n",
        "    df = df.rename(columns=df.iloc[0]).loc[1:].reset_index()\n",
        "    dfs.append(df)\n",
        "\n",
        "  return pd.concat(dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQtXi5GIC6pL"
      },
      "outputs": [],
      "source": [
        "native = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,SERIALNO,SPORDER,CIT&NATIVITY=1\")\n",
        "immigrants = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,SERIALNO,SPORDER,CIT,AGEP,CITWP&NATIVITY=2\")\n",
        "\n",
        "#checked all native-born have CIT = 1, 2, or 3 + all foreign-born have CIT = 4,5\n",
        "print(native['CIT'].unique())\n",
        "print(immigrants['CIT'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqi4OjTSZhhg"
      },
      "outputs": [],
      "source": [
        "#all naturalized were naturalized on or after their approx birth year except\n",
        "#for 19 people naturalized in the 50s when very young\n",
        "naturalized = immigrants[immigrants['CIT'] == \"4\"]\n",
        "correct_years = (2021 - naturalized['AGEP'].astype(int)) <= naturalized['CITWP'].astype(int)+1\n",
        "naturalized['diff'] = naturalized['CITWP'].astype(int) - 2021 + naturalized['AGEP'].astype(int)\n",
        "print(naturalized[~correct_years])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7bGnciLaMoU"
      },
      "outputs": [],
      "source": [
        "#checked all 17+ with ESP = 0 (not own child of householder and not child in subfamily) have NOP = 0\n",
        "no_parents = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,SERIALNO,SPORDER,NOP,ESP,AGEP\", [\"&AGEP=18:99\", \"&ESP=0\"])\n",
        "print((no_parents['NOP'] == 0).all())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19h4ewX18j-W"
      },
      "outputs": [],
      "source": [
        "def print_unique_vals(df, vars):\n",
        "  for col in vars:\n",
        "    print(\"col\")\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UmMYdxlj1E3"
      },
      "source": [
        "## for race/ethnicity vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEe5g0gg2Ssm"
      },
      "outputs": [],
      "source": [
        "#understanding how race vars are determined for someone who is chinese & indian\n",
        "df = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,\" +\n",
        "                  \"SERIALNO,SPORDER,RAC1P,RAC2P,RAC3P,RACBLK\",\n",
        "                   [\"&ANC1P=706&ANC2P=615\", \"&ANC1P=615&ANC2P=706\"])\n",
        "print(df)\n",
        "print_unique_vals(df, [\"RAC1P\", \"RAC2P\", \"RAC3P\", \"RACBLK\"])\n",
        "print(df[df[\"RACBLK\"]==\"1\"])\n",
        "print_unique_vals(df[df[\"RACBLK\"]==\"1\"], [\"RAC1P\", \"RAC2P\", \"RAC3P\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRHkXsVQ7YZn"
      },
      "outputs": [],
      "source": [
        "#looking at \"mixture\" ancestry -- all other ancestry values are single ethnicity\n",
        "df = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,\" +\n",
        "                  \"SERIALNO,SPORDER,RAC1P,RAC2P,RAC3P\",\n",
        "                   [\"&ANC1P=995\", \"&ANC2P=995\"])\n",
        "print_unique_vals(df, [\"RAC1P\", \"RAC2P\", \"RAC3P\"])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78sA0t0Uj4fG"
      },
      "source": [
        "## for income + ethnicity/race"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#average incomes for each race\n",
        "race_avg_income = data.groupby('RACE')['INCOME'].mean().sort_values(ascending=False)\n",
        "race_avg_income_df = race_avg_income.reset_index()\n",
        "race_avg_income_df['RACE'] = race_avg_income_df['RACE'].map(race_mapping)\n",
        "print(race_avg_income_df)"
      ],
      "metadata": {
        "id": "Rxy04Bg_t74Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average incomes for each ethnicity\n",
        "eth_avg_income = data.groupby('ETHNICITY')['INCOME'].mean().sort_values(ascending=False)\n",
        "eth_avg_income_df = eth_avg_income.reset_index()\n",
        "eth_avg_income_df['ETHNICITY'] = eth_avg_income_df['ETHNICITY'].map(ethnicity_mapping)\n",
        "print(eth_avg_income_df)"
      ],
      "metadata": {
        "id": "MibDZXUouWIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBbdpmEAAki3"
      },
      "outputs": [],
      "source": [
        "#checking relationship between specific ETHNICITY (CENSUS) values + income\n",
        "df = request_data(\"https://api.census.gov/data/2021/acs/acs1/pums?get=PWGTP,PINCP,SERIALNO,SPORDER&RAC3P=004&RAC3P=006&RAC3P=013\")\n",
        "df.drop([housing_identifier, person_identifier],axis=1,inplace=True)\n",
        "df = df.astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxxs790rBUU7"
      },
      "outputs": [],
      "source": [
        "#histogram\n",
        "unique = df['RAC3P'].unique()\n",
        "ethnicities = {6:\"filipino\",4:\"indian\",13:\"samoan\"}\n",
        "\n",
        "for val in unique:\n",
        "  new_df = df[df[\"RAC3P\"] == val]\n",
        "  plt.hist(new_df[\"PINCP\"], weights=new_df[\"PWGTP\"], label=ethnicities[val],alpha=.2)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for income + ethnicity/race + age"
      ],
      "metadata": {
        "id": "jY3nLmJpvhCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#todo!"
      ],
      "metadata": {
        "id": "0bxwTsyiyQqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}